{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPxxrOFkAOMr"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from SamplePreprocessor import preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6476,
     "status": "ok",
     "timestamp": 1554294697141,
     "user": {
      "displayName": "Archit Kumar",
      "photoUrl": "",
      "userId": "03572085286762964127"
     },
     "user_tz": -330
    },
    "id": "kqvW14M_Thh6",
    "outputId": "a463e93c-a27c-4d21-d0c4-db80485fb238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdrive\tsample_data  words  words.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEZkC_Mv5FAI"
   },
   "outputs": [],
   "source": [
    "!rm charList.txt\n",
    "!rm corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rnkt9fL2ZGkP"
   },
   "outputs": [],
   "source": [
    "!rm default.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGJeUoNRk_nt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24690,
     "status": "ok",
     "timestamp": 1554292328949,
     "user": {
      "displayName": "Archit Kumar",
      "photoUrl": "",
      "userId": "03572085286762964127"
     },
     "user_tz": -330
    },
    "id": "c4YaZK-PBsUT",
    "outputId": "7b8d44e3-0e70-4c6a-f862-9774d9248672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1964558,
     "output_embedded_package_id": "17PbVv9BmEMnMq7t-Pi5GlwY0bwmTfu0r"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44525,
     "status": "ok",
     "timestamp": 1554292478269,
     "user": {
      "displayName": "Archit Kumar",
      "photoUrl": "",
      "userId": "03572085286762964127"
     },
     "user_tz": -330
    },
    "id": "MUdISD4wB3_s",
    "outputId": "04322585-6945-47ed-ba0d-71be2d4e152a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!unzip \"gdrive/My Drive/Colab Notebooks/data/words.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111,
     "output_embedded_package_id": "1ZwrTukTp5ZR3NdYH1zJHqQUcFFEEkbE0",
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152271,
     "status": "ok",
     "timestamp": 1554293041657,
     "user": {
      "displayName": "Archit Kumar",
      "photoUrl": "",
      "userId": "03572085286762964127"
     },
     "user_tz": -330
    },
    "id": "10Yr8hCfEUDW",
    "outputId": "a4e0663e-159e-4e7e-b5c1-8eabc99e662a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bNfiuPSufCL"
   },
   "outputs": [],
   "source": [
    "def augment_brightness_camera_images(image):\n",
    "#     image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    random_bright = .01+np.random.uniform()\n",
    "    #print(random_bright)\n",
    "    image[:,:] = image[:,:]*random_bright\n",
    "#     image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image\n",
    "\n",
    "def transform_image(pre_img,ang_range,shear_range,trans_range, padding  = 3 , brightness=0,showoff = False):\n",
    "    r , h = pre_img.shape\n",
    "    img = np.ones((r+2*padding, h+2*padding))\n",
    "    img[:,:] = 255\n",
    "    img[padding:r+padding , padding : h+padding] = pre_img[:,:]\n",
    "    \n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    '''\n",
    "    This function transforms images to generate new images.\n",
    "    The function takes in following arguments,\n",
    "    1- Image\n",
    "    2- ang_range: Range of angles for rotation\n",
    "    3- shear_range: Range of values to apply affine transform to\n",
    "    4- trans_range: Range of values to apply translations over.\n",
    "\n",
    "    A Random uniform distribution is used to generate different parameters for transformation\n",
    "\n",
    "    '''\n",
    "    # Rotation\n",
    "\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols= img.shape    \n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "\n",
    "    # Brightness\n",
    "\n",
    "\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "    \n",
    "    if showoff == False :\n",
    "        img = cv2.warpAffine(img,Rot_M,(cols,rows), borderValue=(255,255,255))\n",
    "        img = cv2.warpAffine(img,Trans_M,(cols,rows),borderValue=(255,255,255))\n",
    "        img = cv2.warpAffine(img,shear_M,(cols,rows), borderValue=(255,255,255))\n",
    "    else : \n",
    "        img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "        img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "        img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "\n",
    "\n",
    "    if brightness == 1:\n",
    "      img = augment_brightness_camera_images(img)\n",
    "\n",
    "    return img \n",
    "  \n",
    "def Aug_affine(img):\n",
    "  angle_range = 10\n",
    "  shear_range = 5\n",
    "  trans_range  = 5\n",
    "  return transform_image(img,angle_range , shear_range , trans_range ,padding = 9 ,brightness=1 , showoff = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOxaxQmxuuad"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "# Elastic transform\n",
    "def elastic_transformations(alpha, sigma, rng=np.random.RandomState(42), \n",
    "                            interpolation_order=1):\n",
    "    \"\"\"Returns a function to elastically transform multiple images.\"\"\"\n",
    "    # Good values for:\n",
    "    #   alpha: 2000\n",
    "    #   sigma: between 40 and 60\n",
    "    def _elastic_transform_2D(images):\n",
    "        \"\"\"`images` is a numpy array of shape (K, M, N) of K images of size M*N.\"\"\"\n",
    "        # Take measurements\n",
    "        image_shape = images[0].shape\n",
    "        # Make random fields\n",
    "        dx = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        dy = rng.uniform(-1, 1, image_shape) * alpha\n",
    "        # Smooth dx and dy\n",
    "        sdx = gaussian_filter(dx, sigma=sigma, mode='constant' , cval =  0)\n",
    "        sdy = gaussian_filter(dy, sigma=sigma, mode='constant',  cval =  0)\n",
    "        # Make meshgrid\n",
    "        x, y = np.meshgrid(np.arange(image_shape[1]), np.arange(image_shape[0]))\n",
    "        # Distort meshgrid indices\n",
    "        distorted_indices = (y + sdy).reshape(-1, 1), \\\n",
    "                            (x + sdx).reshape(-1, 1)\n",
    "\n",
    "        # Map cooordinates from image to distorted index set\n",
    "        transformed_images = [map_coordinates(image, distorted_indices, mode='mirror',cval  = 0 ,\n",
    "                                              order= 1).reshape(image_shape)\n",
    "                              for image in images]\n",
    "        return transformed_images\n",
    "    return _elastic_transform_2D\n",
    "\n",
    "def Aug_elastic(img):\n",
    "  est =  elastic_transformations(180,10)\n",
    "  res = est(np.array([img]))\n",
    "  return res[0]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKk7jzo3uupA"
   },
   "outputs": [],
   "source": [
    "def Aug_mix(img):\n",
    "  img = Aug_affine(img)\n",
    "  img = Aug_elastic(img)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "If3pGwsIu0IO"
   },
   "outputs": [],
   "source": [
    "def Aug_paper(img):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQE8RcB7AVBG"
   },
   "outputs": [],
   "source": [
    "def preprocess(img, imgSize, dataAugmentation=False, aug_type  = None):\n",
    "  \n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\tif dataAugmentation == True :\n",
    "# \t\tprint(\"augmentation True\")\n",
    "\t\tif aug_type == \"Affine\":\n",
    "\t\t\timg = Aug_affine(img)\n",
    "\t\telif aug_type == \"Elastic\" :\n",
    "\t\t\timg = Aug_elastic(img)\n",
    "\t\telif aug_type == \"mix\" :\n",
    "\t\t\timg = Aug_mix(img)\n",
    "\t\telif aug_type == \"paper_based\":\n",
    "\t\t\timg  = Aug_paper(img)\n",
    "\t\telse : #default\n",
    "\t\t\tstretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "\t\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "\t\t\timg = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsxLF2LA-2NM"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sample:\n",
    "\t\"sample from the dataset\"\n",
    "\tdef __init__(self, gtText, filePath):\n",
    "\t\tself.gtText = gtText\n",
    "\t\tself.filePath = filePath\n",
    "\n",
    "\n",
    "class Batch:\n",
    "\t\"batch containing images and ground truth texts\"\n",
    "\tdef __init__(self, gtTexts, imgs):\n",
    "\t\tself.imgs = np.stack(imgs, axis=0)\n",
    "\t\tself.gtTexts = gtTexts\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "\tdef __init__(self, filePath, batchSize, imgSize, maxTextLen):\n",
    "\n",
    "\t\tassert filePath[-1]=='/'\n",
    "\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.aug_type = \"default\"\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.batchSize = batchSize\n",
    "\t\tself.imgSize = imgSize\n",
    "\t\tself.samples = []\n",
    "\t\n",
    "\t\tf=open(filePath+'words.txt')\n",
    "\t\tchars = set()\n",
    "\t\tbad_samples = []\n",
    "\t\tbad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "\t\tfor line in f:\n",
    "\t\t\t# ignore comment line\n",
    "\t\t\tif not line or line[0]=='#':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tlineSplit = line.strip().split(' ')\n",
    "\t\t\tassert len(lineSplit) >= 9\n",
    "\t\t\t\n",
    "\t\t\t# filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "\t\t\tfileNameSplit = lineSplit[0].split('-')\n",
    "\t\t\tfileName = filePath + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "\n",
    "\t\t\t# GT text are columns starting at 9\n",
    "\t\t\tgtText = self.truncateLabel(' '.join(lineSplit[8:]), maxTextLen)\n",
    "\t\t\tchars = chars.union(set(list(gtText)))\n",
    "\n",
    "\t\t\t# check if image is not empty\n",
    "\t\t\tif not os.path.getsize(fileName):\n",
    "\t\t\t\tbad_samples.append(lineSplit[0] + '.png')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# put sample into list\n",
    "\t\t\tself.samples.append(Sample(gtText, fileName))\n",
    "\n",
    "\t\t# some images in the IAM dataset are known to be damaged, don't show warning for them\n",
    "\t\tif set(bad_samples) != set(bad_samples_reference):\n",
    "\t\t\tprint(\"Warning, damaged images found:\", bad_samples)\n",
    "\t\t\tprint(\"Damaged images expected:\", bad_samples_reference)\n",
    "\n",
    "\t\t# split into training and validation set: 95% - 5%\n",
    "\t\tsplitIdx = int(0.95 * len(self.samples))\n",
    "\t\tself.trainSamples = self.samples[:splitIdx]\n",
    "\t\tself.validationSamples = self.samples[splitIdx:]\n",
    "\n",
    "\t\t# put words into lists\n",
    "\t\tself.trainWords = [x.gtText for x in self.trainSamples]\n",
    "\t\tself.validationWords = [x.gtText for x in self.validationSamples]\n",
    "\n",
    "\t\t# number of randomly chosen samples per epoch for training \n",
    "\t\tself.numTrainSamplesPerEpoch = 25000 \n",
    "\t\t\n",
    "\t\t# start with train set\n",
    "\t\tself.trainSet()\n",
    "\n",
    "\t\t# list of all chars in dataset\n",
    "\t\tself.charList = sorted(list(chars))\n",
    "\n",
    "\n",
    "\tdef truncateLabel(self, text, maxTextLen):\n",
    "\t\t# ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "\t\t# labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "\t\t# If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "\t\tcost = 0\n",
    "\t\tfor i in range(len(text)):\n",
    "\t\t\tif i != 0 and text[i] == text[i-1]:\n",
    "\t\t\t\tcost += 2\n",
    "\t\t\telse:\n",
    "\t\t\t\tcost += 1\n",
    "\t\t\tif cost > maxTextLen:\n",
    "\t\t\t\treturn text[:i]\n",
    "\t\treturn text\n",
    "\n",
    "\n",
    "\tdef trainSet(self):\n",
    "\t\t\"switch to randomly chosen subset of training set\"\n",
    "\t\tself.dataAugmentation = True\n",
    "\t\tself.currIdx = 0\n",
    "\t\trandom.shuffle(self.trainSamples)\n",
    "\t\tself.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
    "\n",
    "\t\n",
    "\tdef validationSet(self):\n",
    "\t\t\"switch to validation set\"\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.samples = self.validationSamples\n",
    "\n",
    "\n",
    "\tdef getIteratorInfo(self):\n",
    "\t\t\"current batch index and overall number of batches\"\n",
    "\t\treturn (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
    "\n",
    "\n",
    "\tdef hasNext(self):\n",
    "\t\t\"iterator\"\n",
    "\t\treturn self.currIdx + self.batchSize <= len(self.samples)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getNext(self):\n",
    "\t\t\"iterator\"\n",
    "\t\tbatchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "\t\tgtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "\t\timgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "\t\tself.currIdx += self.batchSize\n",
    "\t\treturn Batch(gtTexts, imgs)\n",
    "\t\t\n",
    "# \tdef getNext(self):\n",
    "# \t\t\"iterator\"\n",
    "# \t\tbatchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "\n",
    "# \t\tif self.dataAugmentation == False:\n",
    "# \t\t\tgtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "# \t\t\timgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation , self.aug_type) for i in batchRange]\n",
    "# \t\t\tself.currIdx += self.batchSize\n",
    "# \t\t\treturn Batch(gtTexts, imgs)\n",
    "# \t\tgtTexts = [self.samples[self.currIdx].gtText for i in batchRange]\n",
    "# \t\timgs = [preprocess(cv2.imread(self.samples[self.currIdx].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation, self.aug_type) for i in batchRange]\n",
    "# \t\tself.currIdx += 1\n",
    "# \t\treturn Batch(gtTexts , imgs)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVfpN6k5BO7v"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "\tBestPath = 0\n",
    "\tBeamSearch = 1\n",
    "\tWordBeamSearch = 2\n",
    "\n",
    "\n",
    "class Model: \n",
    "\t\"minimalistic TF model for HTR\"\n",
    "\n",
    "\t# model constants\n",
    "\tbatchSize = 200\n",
    "\timgSize = (128, 32)\n",
    "\tmaxTextLen = 32\n",
    "\n",
    "\tdef __init__(self, charList, decoderType=DecoderType.BestPath, mustRestore=False):\n",
    "\t\t\"init model: add CNN, RNN and CTC and initialize TF\"\n",
    "\t\tself.charList = charList\n",
    "\t\tself.decoderType = decoderType\n",
    "\t\tself.mustRestore = mustRestore\n",
    "\t\tself.snapID = 0\n",
    "\n",
    "\t\t# Whether to use normalization over a batch or a population\n",
    "\t\tself.is_train = tf.placeholder(tf.bool, name=\"is_train\");\n",
    "\n",
    "\t\t# input image batch\n",
    "\t\tself.inputImgs = tf.placeholder(tf.float32, shape=(None, Model.imgSize[0], Model.imgSize[1]))\n",
    "\n",
    "\t\t# setup CNN, RNN and CTC\n",
    "\t\tself.setupCNN()\n",
    "\t\tself.setupRNN()\n",
    "\t\tself.setupCTC()\n",
    "\n",
    "\t\t# setup optimizer to train NN\n",
    "\t\tself.batchesTrained = 0\n",
    "\t\tself.learningRate = tf.placeholder(tf.float32, shape=[])\n",
    "\t\tself.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n",
    "\t\twith tf.control_dependencies(self.update_ops):\n",
    "\t\t\tself.optimizer = tf.train.RMSPropOptimizer(self.learningRate).minimize(self.loss)\n",
    "\n",
    "\t\t# initialize TF\n",
    "\t\t(self.sess, self.saver) = self.setupTF()\n",
    "\n",
    "\t\t\t\n",
    "\tdef setupCNN(self):\n",
    "\t\t\"create CNN layers and return output of these layers\"\n",
    "\t\tcnnIn4d = tf.expand_dims(input=self.inputImgs, axis=3)\n",
    "\n",
    "\t\t# list of parameters for the layers\n",
    "\t\tkernelVals = [5, 5, 3, 3, 3]\n",
    "\t\tfeatureVals = [1, 32, 64, 128, 128, 256]\n",
    "\t\tstrideVals = poolVals = [(2,2), (2,2), (1,2), (1,2), (1,2)]\n",
    "\t\tnumLayers = len(strideVals)\n",
    "\n",
    "\t\t# create layers\n",
    "\t\tpool = cnnIn4d # input to first CNN layer\n",
    "\t\tfor i in range(numLayers):\n",
    "\t\t\tkernel = tf.Variable(tf.truncated_normal([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]], stddev=0.1))\n",
    "\t\t\tconv = tf.nn.conv2d(pool, kernel, padding='SAME',  strides=(1,1,1,1))\n",
    "\t\t\tconv_norm = tf.layers.batch_normalization(conv, training=self.is_train)\n",
    "\t\t\trelu = tf.nn.relu(conv_norm)\n",
    "\t\t\tpool = tf.nn.max_pool(relu, (1, poolVals[i][0], poolVals[i][1], 1), (1, strideVals[i][0], strideVals[i][1], 1), 'VALID')\n",
    "\n",
    "\t\tself.cnnOut4d = pool\n",
    "\n",
    "\n",
    "\tdef setupRNN(self):\n",
    "\t\t\"create RNN layers and return output of these layers\"\n",
    "\t\trnnIn3d = tf.squeeze(self.cnnOut4d, axis=[2])\n",
    "\n",
    "\t\t# basic cells which is used to build RNN\n",
    "\t\tnumHidden = 256\n",
    "\t\tcells = [tf.contrib.rnn.LSTMCell(num_units=numHidden, state_is_tuple=True) for _ in range(2)] # 2 layers\n",
    "\n",
    "\t\t# stack basic cells\n",
    "\t\tstacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "\t\t# bidirectional RNN\n",
    "\t\t# BxTxF -> BxTx2H\n",
    "\t\t((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d, dtype=rnnIn3d.dtype)\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\t# BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "\t\tconcat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\t# project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "\t\tkernel = tf.Variable(tf.truncated_normal([1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n",
    "\t\tself.rnnOut3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "\t\t\n",
    "\n",
    "\tdef setupCTC(self):\n",
    "\t\t\"create CTC loss and decoder and return them\"\n",
    "\t\t# BxTxC -> TxBxC\n",
    "\t\tself.ctcIn3dTBC = tf.transpose(self.rnnOut3d, [1, 0, 2])\n",
    "\t\t# ground truth text as sparse tensor\n",
    "\t\tself.gtTexts = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]) , tf.placeholder(tf.int32, [None]), tf.placeholder(tf.int64, [2]))\n",
    "\n",
    "\t\t# calc loss for batch\n",
    "\t\tself.seqLen = tf.placeholder(tf.int32, [None])\n",
    "\t\tself.loss = tf.reduce_mean(tf.nn.ctc_loss(labels=self.gtTexts, inputs=self.ctcIn3dTBC, sequence_length=self.seqLen, ctc_merge_repeated=True))\n",
    "\n",
    "\t\t# calc loss for each element to compute label probability\n",
    "\t\tself.savedCtcInput = tf.placeholder(tf.float32, shape=[Model.maxTextLen, None, len(self.charList) + 1])\n",
    "\t\tself.lossPerElement = tf.nn.ctc_loss(labels=self.gtTexts, inputs=self.savedCtcInput, sequence_length=self.seqLen, ctc_merge_repeated=True)\n",
    "\n",
    "\t\t# decoder: either best path decoding or beam search decoding\n",
    "\t\tif self.decoderType == DecoderType.BestPath:\n",
    "\t\t\tself.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctcIn3dTBC, sequence_length=self.seqLen)\n",
    "\t\telif self.decoderType == DecoderType.BeamSearch:\n",
    "\t\t\tself.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctcIn3dTBC, sequence_length=self.seqLen, beam_width=50, merge_repeated=False)\n",
    "\t\telif self.decoderType == DecoderType.WordBeamSearch:\n",
    "\t\t\t# import compiled word beam search operation (see https://github.com/githubharald/CTCWordBeamSearch)\n",
    "\t\t\tword_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n",
    "\n",
    "\t\t\t# prepare information about language (dictionary, characters in dataset, characters forming words) \n",
    "\t\t\tchars = str().join(self.charList)\n",
    "\t\t\twordChars = open('../model/wordCharList.txt').read().splitlines()[0]\n",
    "\t\t\tcorpus = open('../data/corpus.txt').read()\n",
    "\n",
    "\t\t\t# decode using the \"Words\" mode of word beam search\n",
    "\t\t\tself.decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(self.ctcIn3dTBC, dim=2), 50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'), wordChars.encode('utf8'))\n",
    "\n",
    "\n",
    "\tdef setupTF(self):\n",
    "\t\t\"initialize TF\"\n",
    "\t\tprint('Python: '+sys.version)\n",
    "\t\tprint('Tensorflow: '+tf.__version__)\n",
    "\n",
    "\t\tsess=tf.Session() # TF session\n",
    "\n",
    "\t\tsaver = tf.train.Saver(max_to_keep=1) # saver saves model to file\n",
    "\t\tmodelDir = '../model/'\n",
    "\t\tlatestSnapshot = tf.train.latest_checkpoint(modelDir) # is there a saved model?\n",
    "\n",
    "\t\t# if model must be restored (for inference), there must be a snapshot\n",
    "\t\tif self.mustRestore and not latestSnapshot:\n",
    "\t\t\traise Exception('No saved model found in: ' + modelDir)\n",
    "\n",
    "\t\t# load saved model if available\n",
    "\t\tif latestSnapshot:\n",
    "\t\t\tprint('Init with stored values from ' + latestSnapshot)\n",
    "\t\t\tsaver.restore(sess, latestSnapshot)\n",
    "\t\telse:\n",
    "\t\t\tprint('Init with new values')\n",
    "\t\t\tsess.run(tf.global_variables_initializer())\n",
    "\n",
    "\t\treturn (sess,saver)\n",
    "\n",
    "\n",
    "\tdef toSparse(self, texts):\n",
    "\t\t\"put ground truth texts into sparse tensor for ctc_loss\"\n",
    "\t\tindices = []\n",
    "\t\tvalues = []\n",
    "\t\tshape = [len(texts), 0] # last entry must be max(labelList[i])\n",
    "\n",
    "\t\t# go over all texts\n",
    "\t\tfor (batchElement, text) in enumerate(texts):\n",
    "\t\t\t# convert to string of label (i.e. class-ids)\n",
    "\t\t\tlabelStr = [self.charList.index(c) for c in text]\n",
    "\t\t\t# sparse tensor must have size of max. label-string\n",
    "\t\t\tif len(labelStr) > shape[1]:\n",
    "\t\t\t\tshape[1] = len(labelStr)\n",
    "\t\t\t# put each label into sparse tensor\n",
    "\t\t\tfor (i, label) in enumerate(labelStr):\n",
    "\t\t\t\tindices.append([batchElement, i])\n",
    "\t\t\t\tvalues.append(label)\n",
    "\n",
    "\t\treturn (indices, values, shape)\n",
    "\n",
    "\n",
    "\tdef decoderOutputToText(self, ctcOutput, batchSize):\n",
    "\t\t\"extract texts from output of CTC decoder\"\n",
    "\t\t\n",
    "\t\t# contains string of labels for each batch element\n",
    "\t\tencodedLabelStrs = [[] for i in range(batchSize)]\n",
    "\n",
    "\t\t# word beam search: label strings terminated by blank\n",
    "\t\tif self.decoderType == DecoderType.WordBeamSearch:\n",
    "\t\t\tblank=len(self.charList)\n",
    "\t\t\tfor b in range(batchSize):\n",
    "\t\t\t\tfor label in ctcOutput[b]:\n",
    "\t\t\t\t\tif label==blank:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tencodedLabelStrs[b].append(label)\n",
    "\n",
    "\t\t# TF decoders: label strings are contained in sparse tensor\n",
    "\t\telse:\n",
    "\t\t\t# ctc returns tuple, first element is SparseTensor \n",
    "\t\t\tdecoded=ctcOutput[0][0] \n",
    "\n",
    "\t\t\t# go over all indices and save mapping: batch -> values\n",
    "\t\t\tidxDict = { b : [] for b in range(batchSize) }\n",
    "\t\t\tfor (idx, idx2d) in enumerate(decoded.indices):\n",
    "\t\t\t\tlabel = decoded.values[idx]\n",
    "\t\t\t\tbatchElement = idx2d[0] # index according to [b,t]\n",
    "\t\t\t\tencodedLabelStrs[batchElement].append(label)\n",
    "\n",
    "\t\t# map labels to chars for all batch elements\n",
    "\t\treturn [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n",
    "\n",
    "\n",
    "\tdef trainBatch(self, batch):\n",
    "\t\t\"feed a batch into the NN to train it\"\n",
    "\t\tnumBatchElements = len(batch.imgs)\n",
    "\t\tsparse = self.toSparse(batch.gtTexts)\n",
    "\t\trate = 0.01 if self.batchesTrained < 10 else (0.001 if self.batchesTrained < 10000 else 0.0001) # decay learning rate\n",
    "\t\tevalList = [self.optimizer, self.loss]\n",
    "\t\tfeedDict = {self.inputImgs : batch.imgs, self.gtTexts : sparse , self.seqLen : [Model.maxTextLen] * numBatchElements, self.learningRate : rate, self.is_train: True}\n",
    "\t\t(_, lossVal) = self.sess.run(evalList, feedDict)\n",
    "\t\tself.batchesTrained += 1\n",
    "\t\treturn lossVal\n",
    "\n",
    "\n",
    "\tdef inferBatch(self, batch, calcProbability=False, probabilityOfGT=False):\n",
    "\t\t\"feed a batch into the NN to recognize the texts\"\n",
    "\t\t\n",
    "\t\t# decode, optionally save RNN output\n",
    "\t\tnumBatchElements = len(batch.imgs)\n",
    "\t\tevalList = [self.decoder] + ([self.ctcIn3dTBC] if calcProbability else [])\n",
    "\t\tfeedDict = {self.inputImgs : batch.imgs, self.seqLen : [Model.maxTextLen] * numBatchElements, self.is_train: False}\n",
    "\t\tevalRes = self.sess.run([self.decoder, self.ctcIn3dTBC], feedDict)\n",
    "\t\tdecoded = evalRes[0]\n",
    "\t\ttexts = self.decoderOutputToText(decoded, numBatchElements)\n",
    "\t\t\n",
    "\t\t# feed RNN output and recognized text into CTC loss to compute labeling probability\n",
    "\t\tprobs = None\n",
    "\t\tif calcProbability:\n",
    "\t\t\tsparse = self.toSparse(batch.gtTexts) if probabilityOfGT else self.toSparse(texts)\n",
    "\t\t\tctcInput = evalRes[1]\n",
    "\t\t\tevalList = self.lossPerElement\n",
    "\t\t\tfeedDict = {self.savedCtcInput : ctcInput, self.gtTexts : sparse, self.seqLen : [Model.maxTextLen] * numBatchElements, self.is_train: False}\n",
    "\t\t\tlossVals = self.sess.run(evalList, feedDict)\n",
    "\t\t\tprobs = np.exp(-lossVals)\n",
    "\t\treturn (texts, probs)\n",
    "\t\n",
    "\n",
    "\tdef save(self):\n",
    "\t\t\"save model to file\"\n",
    "\t\tself.snapID += 1\n",
    "\t\tself.saver.save(self.sess, '../model/snapshot', global_step=self.snapID)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJvWK5rXJpHj"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f34ST39NJtX3"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FilePaths:\n",
    "\t\"filenames and paths to data\"\n",
    "\tfnCharList = 'charList.txt'\n",
    "\tfnAccuracy = 'accuracy.txt'\n",
    "\t#fnTrain = 'data/'\n",
    "\tfnTrain = './'\n",
    "\tfnInfer = 'test.png'\n",
    "\tfnCorpus = 'corpus.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BODljCICJy5S"
   },
   "outputs": [],
   "source": [
    "def train(model, loader):\n",
    "\t\"train NN\"\n",
    "\tepoch = 0 # number of training epochs since start\n",
    "\tbestCharErrorRate = float('inf') # best valdiation character error rate\n",
    "\tnoImprovementSince = 0 # number of epochs no improvement of character error rate occured\n",
    "\tearlyStopping = 5 # stop training after this number of epochs without improvement\n",
    "\tf =  open(\"default.txt\", \"a\")\n",
    " \n",
    "\twhile epoch < 20:\n",
    "\t\tepoch += 1\n",
    "\t\tf.write('Epoch:=============================================================='+ str(epoch)+\"\\n\")    \n",
    "\t\tprint('Epoch:==============================================================', epoch)\n",
    "\n",
    "\t\t# train\n",
    "\t\tf.write('Train NN\\n')\n",
    "\t\tprint('Train NN')\n",
    "\t\tloader.trainSet()\n",
    "\t\twhile loader.hasNext():\n",
    "\t\t\titerInfo = loader.getIteratorInfo()\n",
    "\t\t\tbatch = loader.getNext()\n",
    "\t\t\tloss = model.trainBatch(batch)\n",
    "# \t\t\tf.write('Batch : ' + str( iterInfo[0]) + \"/\" + str(iterInfo[1]) + \"Loss : \" + str(loss) + \"\\n\")\n",
    "\t\t\tprint('Batch:', iterInfo[0],'/', iterInfo[1], 'Loss:', loss)\n",
    "\n",
    "\t\t# validate\n",
    "\t\tf.write(\"validation starts ===========\\n\")\n",
    "\t\tprint(\"validation starts =============\")\n",
    "\t\tcharErrorRate = validate(model, loader,f)\n",
    "\t\tf.write('character error rate : ' + str(charErrorRate) + \"\\n\")\n",
    "\t\tprint('character error rate : ' + str(charErrorRate))\n",
    "\n",
    "\t\t# if best validation accuracy so far, save model parameters\n",
    "\t\tif charErrorRate < bestCharErrorRate:\n",
    "\t\t\tf.write('Character error rate improved, save model' + \"\\n\")\n",
    "\t\t\tprint('Character error rate improved, save model')\n",
    "\t\t\tbestCharErrorRate = charErrorRate\n",
    "\t\t\tnoImprovementSince = 0\n",
    "\t\t\tmodel.save()\n",
    "\t\t\topen(FilePaths.fnAccuracy, 'w').write('Validation character error rate of saved model: %f%%' % (charErrorRate*100.0))\n",
    "\t\telse:\n",
    "\t\t  \tf.write('Character error rate not improved' + \"\\n\")\n",
    "\t\t\tprint('Character error rate not improved')\n",
    "\t\t\tnoImprovementSince += 1\n",
    "\n",
    "\t\t# stop training if no more improvement in the last x epochs\n",
    "\t\tif noImprovementSince >= earlyStopping:\n",
    "\t\t\tf.write('No more improvement since '+ str(earlyStopping) +' epochs. Training stopped.' + \"\\n\")\n",
    "\t\t\tprint('No more improvement since %d epochs. Training stopped.' % earlyStopping)\n",
    "\t\t\tbreak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBSmH3oDJ4xT"
   },
   "outputs": [],
   "source": [
    "def validate(model, loader,file):\n",
    "\t\"validate NN\"\n",
    "\tprint('Validate NN')\n",
    "\tloader.validationSet()\n",
    "\tnumCharErr = 0\n",
    "\tnumCharTotal = 0\n",
    "\tnumWordOK = 0\n",
    "\tnumWordTotal = 0\n",
    "\twhile loader.hasNext():\n",
    "\t\titerInfo = loader.getIteratorInfo()\n",
    "# \t\tprint('Batch:', iterInfo[0],'/', iterInfo[1])\n",
    "\t\tbatch = loader.getNext()\n",
    "\t\t(recognized, _) = model.inferBatch(batch)\n",
    "\t\t\n",
    "\t\t#print('Ground truth -> Recognized')\t\n",
    "\t\tfor i in range(len(recognized)):\n",
    "\t\t\tnumWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "\t\t\tnumWordTotal += 1\n",
    "\t\t\tdist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "\t\t\tnumCharErr += dist\n",
    "\t\t\tnumCharTotal += len(batch.gtTexts[i])\n",
    "\t\t\t#print('[OK]' if dist==0 else '[ERR:%d]' % dist,'\"' + batch.gtTexts[i] + '\"', '->', '\"' + recognized[i] + '\"')\n",
    "\t\n",
    "\t# print validation result\n",
    "\tcharErrorRate = numCharErr / numCharTotal\n",
    "\twordAccuracy = numWordOK / numWordTotal\n",
    "\tfile.write('Character error rate: '+str(charErrorRate*100.0) +' . Word accuracy: '+str(wordAccuracy*100.0)+' .\\n')\n",
    "\tprint('Character error rate: %f%%. Word accuracy: %f%%.' % (charErrorRate*100.0, wordAccuracy*100.0))\n",
    "\treturn charErrorRate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1_Px-bNJ7iF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def infer(model, fnImg):\n",
    "\t\"recognize text in image provided by file path\"\n",
    "\timg = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\n",
    "\tbatch = Batch(None, [img])\n",
    "\t(recognized, probability) = model.inferBatch(batch, True)\n",
    "\tprint('Recognized:', '\"' + recognized[0] + '\"')\n",
    "\tprint('Probability:', probability[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48801
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1084335,
     "status": "error",
     "timestamp": 1554295795048,
     "user": {
      "displayName": "Archit Kumar",
      "photoUrl": "",
      "userId": "03572085286762964127"
     },
     "user_tz": -330
    },
    "id": "XcajlMkcBEVH",
    "outputId": "d3d47b01-8665-4092-c5f1-306cfd345921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 2.7.15rc1 (default, Nov 12 2018, 14:31:15) \n",
      "[GCC 7.3.0]\n",
      "Tensorflow: 1.13.1\n",
      "Init with new values\n",
      "Epoch:============================================================== 1\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 126.986305\n",
      "Batch: 2 / 125 Loss: 28.466238\n",
      "Batch: 3 / 125 Loss: 30.084763\n",
      "Batch: 4 / 125 Loss: 20.057646\n",
      "Batch: 5 / 125 Loss: 18.239515\n",
      "Batch: 6 / 125 Loss: 19.27141\n",
      "Batch: 7 / 125 Loss: 18.237785\n",
      "Batch: 8 / 125 Loss: 17.72989\n",
      "Batch: 9 / 125 Loss: 17.869884\n",
      "Batch: 10 / 125 Loss: 19.348923\n",
      "Batch: 11 / 125 Loss: 20.549566\n",
      "Batch: 12 / 125 Loss: 17.57199\n",
      "Batch: 13 / 125 Loss: 17.228159\n",
      "Batch: 14 / 125 Loss: 16.226202\n",
      "Batch: 15 / 125 Loss: 16.835302\n",
      "Batch: 16 / 125 Loss: 17.054392\n",
      "Batch: 17 / 125 Loss: 17.395758\n",
      "Batch: 18 / 125 Loss: 15.320575\n",
      "Batch: 19 / 125 Loss: 16.670359\n",
      "Batch: 20 / 125 Loss: 16.708569\n",
      "Batch: 21 / 125 Loss: 15.034839\n",
      "Batch: 22 / 125 Loss: 15.689043\n",
      "Batch: 23 / 125 Loss: 15.563081\n",
      "Batch: 24 / 125 Loss: 15.668609\n",
      "Batch: 25 / 125 Loss: 16.186586\n",
      "Batch: 26 / 125 Loss: 16.755997\n",
      "Batch: 27 / 125 Loss: 15.1267185\n",
      "Batch: 28 / 125 Loss: 16.13783\n",
      "Batch: 29 / 125 Loss: 16.143936\n",
      "Batch: 30 / 125 Loss: 15.674488\n",
      "Batch: 31 / 125 Loss: 15.986171\n",
      "Batch: 32 / 125 Loss: 16.252075\n",
      "Batch: 33 / 125 Loss: 16.24813\n",
      "Batch: 34 / 125 Loss: 16.699512\n",
      "Batch: 35 / 125 Loss: 16.61755\n",
      "Batch: 36 / 125 Loss: 16.995125\n",
      "Batch: 37 / 125 Loss: 15.671379\n",
      "Batch: 38 / 125 Loss: 14.8804245\n",
      "Batch: 39 / 125 Loss: 15.852087\n",
      "Batch: 40 / 125 Loss: 15.851929\n",
      "Batch: 41 / 125 Loss: 14.362995\n",
      "Batch: 42 / 125 Loss: 16.437134\n",
      "Batch: 43 / 125 Loss: 14.902227\n",
      "Batch: 44 / 125 Loss: 15.779911\n",
      "Batch: 45 / 125 Loss: 15.991273\n",
      "Batch: 46 / 125 Loss: 16.032856\n",
      "Batch: 47 / 125 Loss: 16.13819\n",
      "Batch: 48 / 125 Loss: 16.345243\n",
      "Batch: 49 / 125 Loss: 14.733916\n",
      "Batch: 50 / 125 Loss: 16.711334\n",
      "Batch: 51 / 125 Loss: 14.527492\n",
      "Batch: 52 / 125 Loss: 14.561758\n",
      "Batch: 53 / 125 Loss: 15.153892\n",
      "Batch: 54 / 125 Loss: 15.000157\n",
      "Batch: 55 / 125 Loss: 15.339778\n",
      "Batch: 56 / 125 Loss: 15.3354025\n",
      "Batch: 57 / 125 Loss: 15.350055\n",
      "Batch: 58 / 125 Loss: 16.274569\n",
      "Batch: 59 / 125 Loss: 16.000854\n",
      "Batch: 60 / 125 Loss: 17.064957\n",
      "Batch: 61 / 125 Loss: 16.05581\n",
      "Batch: 62 / 125 Loss: 14.638023\n",
      "Batch: 63 / 125 Loss: 14.244667\n",
      "Batch: 64 / 125 Loss: 14.979884\n",
      "Batch: 65 / 125 Loss: 16.01408\n",
      "Batch: 66 / 125 Loss: 15.1891365\n",
      "Batch: 67 / 125 Loss: 15.145034\n",
      "Batch: 68 / 125 Loss: 15.76664\n",
      "Batch: 69 / 125 Loss: 15.175388\n",
      "Batch: 70 / 125 Loss: 15.520531\n",
      "Batch: 71 / 125 Loss: 14.295579\n",
      "Batch: 72 / 125 Loss: 14.855586\n",
      "Batch: 73 / 125 Loss: 14.534521\n",
      "Batch: 74 / 125 Loss: 14.391516\n",
      "Batch: 75 / 125 Loss: 13.896636\n",
      "Batch: 76 / 125 Loss: 14.957597\n",
      "Batch: 77 / 125 Loss: 15.68046\n",
      "Batch: 78 / 125 Loss: 14.944817\n",
      "Batch: 79 / 125 Loss: 14.710358\n",
      "Batch: 80 / 125 Loss: 15.746612\n",
      "Batch: 81 / 125 Loss: 14.992719\n",
      "Batch: 82 / 125 Loss: 16.766962\n",
      "Batch: 83 / 125 Loss: 15.435734\n",
      "Batch: 84 / 125 Loss: 14.797207\n",
      "Batch: 85 / 125 Loss: 13.547721\n",
      "Batch: 86 / 125 Loss: 14.353946\n",
      "Batch: 87 / 125 Loss: 15.119807\n",
      "Batch: 88 / 125 Loss: 13.987621\n",
      "Batch: 89 / 125 Loss: 14.121064\n",
      "Batch: 90 / 125 Loss: 14.693045\n",
      "Batch: 91 / 125 Loss: 14.855896\n",
      "Batch: 92 / 125 Loss: 15.894951\n",
      "Batch: 93 / 125 Loss: 15.929241\n",
      "Batch: 94 / 125 Loss: 16.64588\n",
      "Batch: 95 / 125 Loss: 15.003027\n",
      "Batch: 96 / 125 Loss: 15.413526\n",
      "Batch: 97 / 125 Loss: 15.213705\n",
      "Batch: 98 / 125 Loss: 15.300985\n",
      "Batch: 99 / 125 Loss: 13.9003725\n",
      "Batch: 100 / 125 Loss: 15.699499\n",
      "Batch: 101 / 125 Loss: 14.802995\n",
      "Batch: 102 / 125 Loss: 14.876701\n",
      "Batch: 103 / 125 Loss: 15.125449\n",
      "Batch: 104 / 125 Loss: 13.5344925\n",
      "Batch: 105 / 125 Loss: 14.471645\n",
      "Batch: 106 / 125 Loss: 14.466654\n",
      "Batch: 107 / 125 Loss: 13.790877\n",
      "Batch: 108 / 125 Loss: 15.814701\n",
      "Batch: 109 / 125 Loss: 14.3159275\n",
      "Batch: 110 / 125 Loss: 15.245685\n",
      "Batch: 111 / 125 Loss: 13.646061\n",
      "Batch: 112 / 125 Loss: 15.102217\n",
      "Batch: 113 / 125 Loss: 15.509704\n",
      "Batch: 114 / 125 Loss: 13.577207\n",
      "Batch: 115 / 125 Loss: 13.587659\n",
      "Batch: 116 / 125 Loss: 14.90857\n",
      "Batch: 117 / 125 Loss: 13.901894\n",
      "Batch: 118 / 125 Loss: 14.111639\n",
      "Batch: 119 / 125 Loss: 13.993024\n",
      "Batch: 120 / 125 Loss: 14.997593\n",
      "Batch: 121 / 125 Loss: 14.641131\n",
      "Batch: 122 / 125 Loss: 14.648282\n",
      "Batch: 123 / 125 Loss: 14.192477\n",
      "Batch: 124 / 125 Loss: 14.001826\n",
      "Batch: 125 / 125 Loss: 13.98302\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 93.675745%. Word accuracy: 0.000000%.\n",
      "character error rate : 0.936757448364\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 2\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 13.788765\n",
      "Batch: 2 / 125 Loss: 12.944121\n",
      "Batch: 3 / 125 Loss: 14.371587\n",
      "Batch: 4 / 125 Loss: 12.752594\n",
      "Batch: 5 / 125 Loss: 15.597367\n",
      "Batch: 6 / 125 Loss: 15.333636\n",
      "Batch: 7 / 125 Loss: 14.693923\n",
      "Batch: 8 / 125 Loss: 14.029366\n",
      "Batch: 9 / 125 Loss: 13.384805\n",
      "Batch: 10 / 125 Loss: 14.60899\n",
      "Batch: 11 / 125 Loss: 14.6892185\n",
      "Batch: 12 / 125 Loss: 13.736389\n",
      "Batch: 13 / 125 Loss: 14.698492\n",
      "Batch: 14 / 125 Loss: 14.877175\n",
      "Batch: 15 / 125 Loss: 14.340942\n",
      "Batch: 16 / 125 Loss: 13.677569\n",
      "Batch: 17 / 125 Loss: 13.590089\n",
      "Batch: 18 / 125 Loss: 14.119595\n",
      "Batch: 19 / 125 Loss: 13.356217\n",
      "Batch: 20 / 125 Loss: 12.851786\n",
      "Batch: 21 / 125 Loss: 13.274736\n",
      "Batch: 22 / 125 Loss: 12.634081\n",
      "Batch: 23 / 125 Loss: 14.104575\n",
      "Batch: 24 / 125 Loss: 14.890415\n",
      "Batch: 25 / 125 Loss: 16.507933\n",
      "Batch: 26 / 125 Loss: 13.814371\n",
      "Batch: 27 / 125 Loss: 14.200382\n",
      "Batch: 28 / 125 Loss: 13.2024145\n",
      "Batch: 29 / 125 Loss: 12.097133\n",
      "Batch: 30 / 125 Loss: 11.717445\n",
      "Batch: 31 / 125 Loss: 13.337741\n",
      "Batch: 32 / 125 Loss: 13.458392\n",
      "Batch: 33 / 125 Loss: 12.977443\n",
      "Batch: 34 / 125 Loss: 13.090484\n",
      "Batch: 35 / 125 Loss: 13.499513\n",
      "Batch: 36 / 125 Loss: 13.114972\n",
      "Batch: 37 / 125 Loss: 13.809095\n",
      "Batch: 38 / 125 Loss: 13.633176\n",
      "Batch: 39 / 125 Loss: 13.533748\n",
      "Batch: 40 / 125 Loss: 13.961958\n",
      "Batch: 41 / 125 Loss: 14.379945\n",
      "Batch: 42 / 125 Loss: 13.980954\n",
      "Batch: 43 / 125 Loss: 13.681632\n",
      "Batch: 44 / 125 Loss: 13.537902\n",
      "Batch: 45 / 125 Loss: 12.883786\n",
      "Batch: 46 / 125 Loss: 12.967263\n",
      "Batch: 47 / 125 Loss: 11.644912\n",
      "Batch: 48 / 125 Loss: 12.538844\n",
      "Batch: 49 / 125 Loss: 13.461224\n",
      "Batch: 50 / 125 Loss: 12.248977\n",
      "Batch: 51 / 125 Loss: 12.80206\n",
      "Batch: 52 / 125 Loss: 13.427727\n",
      "Batch: 53 / 125 Loss: 12.649497\n",
      "Batch: 54 / 125 Loss: 12.078109\n",
      "Batch: 55 / 125 Loss: 14.561199\n",
      "Batch: 56 / 125 Loss: 12.577084\n",
      "Batch: 57 / 125 Loss: 14.048492\n",
      "Batch: 58 / 125 Loss: 13.007921\n",
      "Batch: 59 / 125 Loss: 12.336335\n",
      "Batch: 60 / 125 Loss: 12.447414\n",
      "Batch: 61 / 125 Loss: 11.827108\n",
      "Batch: 62 / 125 Loss: 13.616005\n",
      "Batch: 63 / 125 Loss: 12.09889\n",
      "Batch: 64 / 125 Loss: 11.4566345\n",
      "Batch: 65 / 125 Loss: 11.405283\n",
      "Batch: 66 / 125 Loss: 11.9354725\n",
      "Batch: 67 / 125 Loss: 12.123172\n",
      "Batch: 68 / 125 Loss: 11.468098\n",
      "Batch: 69 / 125 Loss: 12.222745\n",
      "Batch: 70 / 125 Loss: 13.159018\n",
      "Batch: 71 / 125 Loss: 13.523362\n",
      "Batch: 72 / 125 Loss: 13.048527\n",
      "Batch: 73 / 125 Loss: 13.001691\n",
      "Batch: 74 / 125 Loss: 13.102545\n",
      "Batch: 75 / 125 Loss: 13.8453665\n",
      "Batch: 76 / 125 Loss: 12.652332\n",
      "Batch: 77 / 125 Loss: 11.827038\n",
      "Batch: 78 / 125 Loss: 12.277616\n",
      "Batch: 79 / 125 Loss: 11.667883\n",
      "Batch: 80 / 125 Loss: 12.796191\n",
      "Batch: 81 / 125 Loss: 11.9781685\n",
      "Batch: 82 / 125 Loss: 12.5041685\n",
      "Batch: 83 / 125 Loss: 12.249243\n",
      "Batch: 84 / 125 Loss: 12.109567\n",
      "Batch: 85 / 125 Loss: 12.456715\n",
      "Batch: 86 / 125 Loss: 11.107798\n",
      "Batch: 87 / 125 Loss: 13.556544\n",
      "Batch: 88 / 125 Loss: 12.647277\n",
      "Batch: 89 / 125 Loss: 11.486616\n",
      "Batch: 90 / 125 Loss: 11.966001\n",
      "Batch: 91 / 125 Loss: 12.640866\n",
      "Batch: 92 / 125 Loss: 11.739715\n",
      "Batch: 93 / 125 Loss: 11.786886\n",
      "Batch: 94 / 125 Loss: 11.369699\n",
      "Batch: 95 / 125 Loss: 12.607621\n",
      "Batch: 96 / 125 Loss: 12.214483\n",
      "Batch: 97 / 125 Loss: 12.633293\n",
      "Batch: 98 / 125 Loss: 12.682937\n",
      "Batch: 99 / 125 Loss: 12.508811\n",
      "Batch: 100 / 125 Loss: 12.169556\n",
      "Batch: 101 / 125 Loss: 11.269696\n",
      "Batch: 102 / 125 Loss: 10.645038\n",
      "Batch: 103 / 125 Loss: 11.684933\n",
      "Batch: 104 / 125 Loss: 10.377663\n",
      "Batch: 105 / 125 Loss: 12.201786\n",
      "Batch: 106 / 125 Loss: 11.659759\n",
      "Batch: 107 / 125 Loss: 11.270337\n",
      "Batch: 108 / 125 Loss: 11.214979\n",
      "Batch: 109 / 125 Loss: 11.399144\n",
      "Batch: 110 / 125 Loss: 11.386209\n",
      "Batch: 111 / 125 Loss: 11.068214\n",
      "Batch: 112 / 125 Loss: 11.497514\n",
      "Batch: 113 / 125 Loss: 12.080441\n",
      "Batch: 114 / 125 Loss: 10.790947\n",
      "Batch: 115 / 125 Loss: 11.598894\n",
      "Batch: 116 / 125 Loss: 10.360939\n",
      "Batch: 117 / 125 Loss: 11.171552\n",
      "Batch: 118 / 125 Loss: 10.741469\n",
      "Batch: 119 / 125 Loss: 13.036207\n",
      "Batch: 120 / 125 Loss: 11.247589\n",
      "Batch: 121 / 125 Loss: 11.821953\n",
      "Batch: 122 / 125 Loss: 11.470822\n",
      "Batch: 123 / 125 Loss: 12.324784\n",
      "Batch: 124 / 125 Loss: 11.520404\n",
      "Batch: 125 / 125 Loss: 10.855535\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 83.037836%. Word accuracy: 10.535714%.\n",
      "character error rate : 0.830378358618\n",
      "Character error rate improved, save model\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch:============================================================== 3\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 11.079321\n",
      "Batch: 2 / 125 Loss: 10.825862\n",
      "Batch: 3 / 125 Loss: 11.880409\n",
      "Batch: 4 / 125 Loss: 11.283011\n",
      "Batch: 5 / 125 Loss: 11.226571\n",
      "Batch: 6 / 125 Loss: 10.009657\n",
      "Batch: 7 / 125 Loss: 10.284922\n",
      "Batch: 8 / 125 Loss: 11.038938\n",
      "Batch: 9 / 125 Loss: 11.021188\n",
      "Batch: 10 / 125 Loss: 10.95232\n",
      "Batch: 11 / 125 Loss: 10.567102\n",
      "Batch: 12 / 125 Loss: 12.501818\n",
      "Batch: 13 / 125 Loss: 11.884848\n",
      "Batch: 14 / 125 Loss: 11.211917\n",
      "Batch: 15 / 125 Loss: 10.9101925\n",
      "Batch: 16 / 125 Loss: 10.283396\n",
      "Batch: 17 / 125 Loss: 10.5658\n",
      "Batch: 18 / 125 Loss: 10.285803\n",
      "Batch: 19 / 125 Loss: 11.596231\n",
      "Batch: 20 / 125 Loss: 9.977179\n",
      "Batch: 21 / 125 Loss: 10.860722\n",
      "Batch: 22 / 125 Loss: 10.758457\n",
      "Batch: 23 / 125 Loss: 10.263995\n",
      "Batch: 24 / 125 Loss: 9.669986\n",
      "Batch: 25 / 125 Loss: 11.422246\n",
      "Batch: 26 / 125 Loss: 10.139656\n",
      "Batch: 27 / 125 Loss: 11.225923\n",
      "Batch: 28 / 125 Loss: 12.164637\n",
      "Batch: 29 / 125 Loss: 10.682155\n",
      "Batch: 30 / 125 Loss: 9.841182\n",
      "Batch: 31 / 125 Loss: 9.780453\n",
      "Batch: 32 / 125 Loss: 10.221734\n",
      "Batch: 33 / 125 Loss: 8.906951\n",
      "Batch: 34 / 125 Loss: 10.593464\n",
      "Batch: 35 / 125 Loss: 10.619075\n",
      "Batch: 36 / 125 Loss: 10.752998\n",
      "Batch: 37 / 125 Loss: 11.121978\n",
      "Batch: 38 / 125 Loss: 9.898388\n",
      "Batch: 39 / 125 Loss: 11.031089\n",
      "Batch: 40 / 125 Loss: 11.13302\n",
      "Batch: 41 / 125 Loss: 10.207441\n",
      "Batch: 42 / 125 Loss: 10.239784\n",
      "Batch: 43 / 125 Loss: 10.012403\n",
      "Batch: 44 / 125 Loss: 8.970086\n",
      "Batch: 45 / 125 Loss: 9.414051\n",
      "Batch: 46 / 125 Loss: 9.396684\n",
      "Batch: 47 / 125 Loss: 9.5372505\n",
      "Batch: 48 / 125 Loss: 9.451287\n",
      "Batch: 49 / 125 Loss: 9.934626\n",
      "Batch: 50 / 125 Loss: 8.920019\n",
      "Batch: 51 / 125 Loss: 9.148502\n",
      "Batch: 52 / 125 Loss: 11.118039\n",
      "Batch: 53 / 125 Loss: 10.560616\n",
      "Batch: 54 / 125 Loss: 10.962334\n",
      "Batch: 55 / 125 Loss: 10.49013\n",
      "Batch: 56 / 125 Loss: 10.053747\n",
      "Batch: 57 / 125 Loss: 10.346465\n",
      "Batch: 58 / 125 Loss: 10.233519\n",
      "Batch: 59 / 125 Loss: 9.26524\n",
      "Batch: 60 / 125 Loss: 11.408243\n",
      "Batch: 61 / 125 Loss: 9.918327\n",
      "Batch: 62 / 125 Loss: 10.823599\n",
      "Batch: 63 / 125 Loss: 10.288012\n",
      "Batch: 64 / 125 Loss: 9.59648\n",
      "Batch: 65 / 125 Loss: 9.472257\n",
      "Batch: 66 / 125 Loss: 9.235691\n",
      "Batch: 67 / 125 Loss: 9.264758\n",
      "Batch: 68 / 125 Loss: 8.946697\n",
      "Batch: 69 / 125 Loss: 8.39722\n",
      "Batch: 70 / 125 Loss: 9.304006\n",
      "Batch: 71 / 125 Loss: 8.283189\n",
      "Batch: 72 / 125 Loss: 10.981331\n",
      "Batch: 73 / 125 Loss: 9.528628\n",
      "Batch: 74 / 125 Loss: 8.750173\n",
      "Batch: 75 / 125 Loss: 9.19837\n",
      "Batch: 76 / 125 Loss: 9.369031\n",
      "Batch: 77 / 125 Loss: 10.3766\n",
      "Batch: 78 / 125 Loss: 8.735649\n",
      "Batch: 79 / 125 Loss: 9.595194\n",
      "Batch: 80 / 125 Loss: 10.430245\n",
      "Batch: 81 / 125 Loss: 8.901161\n",
      "Batch: 82 / 125 Loss: 8.776512\n",
      "Batch: 83 / 125 Loss: 9.417636\n",
      "Batch: 84 / 125 Loss: 8.706376\n",
      "Batch: 85 / 125 Loss: 9.440656\n",
      "Batch: 86 / 125 Loss: 9.437933\n",
      "Batch: 87 / 125 Loss: 9.7293625\n",
      "Batch: 88 / 125 Loss: 10.370411\n",
      "Batch: 89 / 125 Loss: 9.181004\n",
      "Batch: 90 / 125 Loss: 7.129307\n",
      "Batch: 91 / 125 Loss: 10.662977\n",
      "Batch: 92 / 125 Loss: 10.548005\n",
      "Batch: 93 / 125 Loss: 10.049261\n",
      "Batch: 94 / 125 Loss: 9.886523\n",
      "Batch: 95 / 125 Loss: 9.192112\n",
      "Batch: 96 / 125 Loss: 9.953677\n",
      "Batch: 97 / 125 Loss: 9.055418\n",
      "Batch: 98 / 125 Loss: 8.837626\n",
      "Batch: 99 / 125 Loss: 9.841049\n",
      "Batch: 100 / 125 Loss: 9.552019\n",
      "Batch: 101 / 125 Loss: 9.832226\n",
      "Batch: 102 / 125 Loss: 9.11476\n",
      "Batch: 103 / 125 Loss: 8.500433\n",
      "Batch: 104 / 125 Loss: 9.954953\n",
      "Batch: 105 / 125 Loss: 8.977039\n",
      "Batch: 106 / 125 Loss: 12.322399\n",
      "Batch: 107 / 125 Loss: 10.741276\n",
      "Batch: 108 / 125 Loss: 8.994965\n",
      "Batch: 109 / 125 Loss: 8.346119\n",
      "Batch: 110 / 125 Loss: 8.952796\n",
      "Batch: 111 / 125 Loss: 9.756574\n",
      "Batch: 112 / 125 Loss: 9.516105\n",
      "Batch: 113 / 125 Loss: 8.800675\n",
      "Batch: 114 / 125 Loss: 7.906099\n",
      "Batch: 115 / 125 Loss: 9.290428\n",
      "Batch: 116 / 125 Loss: 9.032272\n",
      "Batch: 117 / 125 Loss: 8.849985\n",
      "Batch: 118 / 125 Loss: 8.878663\n",
      "Batch: 119 / 125 Loss: 8.071039\n",
      "Batch: 120 / 125 Loss: 8.693494\n",
      "Batch: 121 / 125 Loss: 8.728186\n",
      "Batch: 122 / 125 Loss: 8.037209\n",
      "Batch: 123 / 125 Loss: 9.449133\n",
      "Batch: 124 / 125 Loss: 8.286618\n",
      "Batch: 125 / 125 Loss: 8.759324\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 69.598794%. Word accuracy: 20.571429%.\n",
      "character error rate : 0.695987936392\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 4\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 8.808428\n",
      "Batch: 2 / 125 Loss: 8.836343\n",
      "Batch: 3 / 125 Loss: 8.171739\n",
      "Batch: 4 / 125 Loss: 8.403004\n",
      "Batch: 5 / 125 Loss: 8.382185\n",
      "Batch: 6 / 125 Loss: 8.601478\n",
      "Batch: 7 / 125 Loss: 9.785386\n",
      "Batch: 8 / 125 Loss: 8.297723\n",
      "Batch: 9 / 125 Loss: 9.794815\n",
      "Batch: 10 / 125 Loss: 8.908563\n",
      "Batch: 11 / 125 Loss: 8.971201\n",
      "Batch: 12 / 125 Loss: 8.653585\n",
      "Batch: 13 / 125 Loss: 8.632406\n",
      "Batch: 14 / 125 Loss: 9.645395\n",
      "Batch: 15 / 125 Loss: 7.7537193\n",
      "Batch: 16 / 125 Loss: 9.22688\n",
      "Batch: 17 / 125 Loss: 8.317326\n",
      "Batch: 18 / 125 Loss: 7.6551337\n",
      "Batch: 19 / 125 Loss: 9.082964\n",
      "Batch: 20 / 125 Loss: 8.572806\n",
      "Batch: 21 / 125 Loss: 9.602311\n",
      "Batch: 22 / 125 Loss: 8.986981\n",
      "Batch: 23 / 125 Loss: 9.341096\n",
      "Batch: 24 / 125 Loss: 8.961734\n",
      "Batch: 25 / 125 Loss: 9.21275\n",
      "Batch: 26 / 125 Loss: 8.854954\n",
      "Batch: 27 / 125 Loss: 8.196111\n",
      "Batch: 28 / 125 Loss: 8.890586\n",
      "Batch: 29 / 125 Loss: 8.847192\n",
      "Batch: 30 / 125 Loss: 8.059892\n",
      "Batch: 31 / 125 Loss: 8.761689\n",
      "Batch: 32 / 125 Loss: 8.6568365\n",
      "Batch: 33 / 125 Loss: 9.114582\n",
      "Batch: 34 / 125 Loss: 8.492649\n",
      "Batch: 35 / 125 Loss: 8.179893\n",
      "Batch: 36 / 125 Loss: 8.411474\n",
      "Batch: 37 / 125 Loss: 7.6488776\n",
      "Batch: 38 / 125 Loss: 8.503385\n",
      "Batch: 39 / 125 Loss: 7.278168\n",
      "Batch: 40 / 125 Loss: 9.201071\n",
      "Batch: 41 / 125 Loss: 8.262635\n",
      "Batch: 42 / 125 Loss: 9.216012\n",
      "Batch: 43 / 125 Loss: 8.430119\n",
      "Batch: 44 / 125 Loss: 6.972028\n",
      "Batch: 45 / 125 Loss: 7.9016304\n",
      "Batch: 46 / 125 Loss: 8.276241\n",
      "Batch: 47 / 125 Loss: 8.272791\n",
      "Batch: 48 / 125 Loss: 7.589682\n",
      "Batch: 49 / 125 Loss: 8.843006\n",
      "Batch: 50 / 125 Loss: 8.914467\n",
      "Batch: 51 / 125 Loss: 8.112921\n",
      "Batch: 52 / 125 Loss: 7.082603\n",
      "Batch: 53 / 125 Loss: 7.2014575\n",
      "Batch: 54 / 125 Loss: 7.859766\n",
      "Batch: 55 / 125 Loss: 7.9742785\n",
      "Batch: 56 / 125 Loss: 8.963258\n",
      "Batch: 57 / 125 Loss: 7.7342434\n",
      "Batch: 58 / 125 Loss: 8.520446\n",
      "Batch: 59 / 125 Loss: 8.959812\n",
      "Batch: 60 / 125 Loss: 8.149982\n",
      "Batch: 61 / 125 Loss: 7.5507793\n",
      "Batch: 62 / 125 Loss: 9.235503\n",
      "Batch: 63 / 125 Loss: 8.733874\n",
      "Batch: 64 / 125 Loss: 9.043495\n",
      "Batch: 65 / 125 Loss: 7.6225734\n",
      "Batch: 66 / 125 Loss: 8.891635\n",
      "Batch: 67 / 125 Loss: 7.3712163\n",
      "Batch: 68 / 125 Loss: 8.098297\n",
      "Batch: 69 / 125 Loss: 8.616932\n",
      "Batch: 70 / 125 Loss: 7.879862\n",
      "Batch: 71 / 125 Loss: 7.7875404\n",
      "Batch: 72 / 125 Loss: 8.26405\n",
      "Batch: 73 / 125 Loss: 8.329115\n",
      "Batch: 74 / 125 Loss: 7.7091284\n",
      "Batch: 75 / 125 Loss: 7.071186\n",
      "Batch: 76 / 125 Loss: 7.0642247\n",
      "Batch: 77 / 125 Loss: 7.96645\n",
      "Batch: 78 / 125 Loss: 7.6543593\n",
      "Batch: 79 / 125 Loss: 7.4793043\n",
      "Batch: 80 / 125 Loss: 7.8614173\n",
      "Batch: 81 / 125 Loss: 8.512317\n",
      "Batch: 82 / 125 Loss: 7.5210643\n",
      "Batch: 83 / 125 Loss: 8.017989\n",
      "Batch: 84 / 125 Loss: 8.514358\n",
      "Batch: 85 / 125 Loss: 9.128457\n",
      "Batch: 86 / 125 Loss: 8.383451\n",
      "Batch: 87 / 125 Loss: 8.118755\n",
      "Batch: 88 / 125 Loss: 8.201932\n",
      "Batch: 89 / 125 Loss: 8.140371\n",
      "Batch: 90 / 125 Loss: 8.479746\n",
      "Batch: 91 / 125 Loss: 7.428452\n",
      "Batch: 92 / 125 Loss: 7.975012\n",
      "Batch: 93 / 125 Loss: 7.66832\n",
      "Batch: 94 / 125 Loss: 7.3658423\n",
      "Batch: 95 / 125 Loss: 8.690968\n",
      "Batch: 96 / 125 Loss: 8.059062\n",
      "Batch: 97 / 125 Loss: 7.2961106\n",
      "Batch: 98 / 125 Loss: 8.148094\n",
      "Batch: 99 / 125 Loss: 7.8173537\n",
      "Batch: 100 / 125 Loss: 7.185177\n",
      "Batch: 101 / 125 Loss: 7.9919105\n",
      "Batch: 102 / 125 Loss: 7.930841\n",
      "Batch: 103 / 125 Loss: 6.11878\n",
      "Batch: 104 / 125 Loss: 8.92642\n",
      "Batch: 105 / 125 Loss: 7.57779\n",
      "Batch: 106 / 125 Loss: 7.0220857\n",
      "Batch: 107 / 125 Loss: 6.941546\n",
      "Batch: 108 / 125 Loss: 7.2682686\n",
      "Batch: 109 / 125 Loss: 7.3105745\n",
      "Batch: 110 / 125 Loss: 7.3899574\n",
      "Batch: 111 / 125 Loss: 7.447968\n",
      "Batch: 112 / 125 Loss: 7.2529707\n",
      "Batch: 113 / 125 Loss: 7.16954\n",
      "Batch: 114 / 125 Loss: 7.5995073\n",
      "Batch: 115 / 125 Loss: 7.8179283\n",
      "Batch: 116 / 125 Loss: 7.2798405\n",
      "Batch: 117 / 125 Loss: 7.594262\n",
      "Batch: 118 / 125 Loss: 6.83847\n",
      "Batch: 119 / 125 Loss: 6.5648375\n",
      "Batch: 120 / 125 Loss: 6.5780883\n",
      "Batch: 121 / 125 Loss: 7.040028\n",
      "Batch: 122 / 125 Loss: 6.8310933\n",
      "Batch: 123 / 125 Loss: 7.302244\n",
      "Batch: 124 / 125 Loss: 7.100289\n",
      "Batch: 125 / 125 Loss: 7.66963\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 61.131420%. Word accuracy: 22.357143%.\n",
      "character error rate : 0.611314202157\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 5\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 7.435109\n",
      "Batch: 2 / 125 Loss: 6.985325\n",
      "Batch: 3 / 125 Loss: 6.7038207\n",
      "Batch: 4 / 125 Loss: 7.256947\n",
      "Batch: 5 / 125 Loss: 6.9651165\n",
      "Batch: 6 / 125 Loss: 7.3930516\n",
      "Batch: 7 / 125 Loss: 6.9577336\n",
      "Batch: 8 / 125 Loss: 6.6059403\n",
      "Batch: 9 / 125 Loss: 6.598301\n",
      "Batch: 10 / 125 Loss: 8.314037\n",
      "Batch: 11 / 125 Loss: 7.2316823\n",
      "Batch: 12 / 125 Loss: 8.099902\n",
      "Batch: 13 / 125 Loss: 7.496008\n",
      "Batch: 14 / 125 Loss: 7.343003\n",
      "Batch: 15 / 125 Loss: 6.246501\n",
      "Batch: 16 / 125 Loss: 6.5677147\n",
      "Batch: 17 / 125 Loss: 6.6976414\n",
      "Batch: 18 / 125 Loss: 7.867724\n",
      "Batch: 19 / 125 Loss: 7.595758\n",
      "Batch: 20 / 125 Loss: 7.893479\n",
      "Batch: 21 / 125 Loss: 7.861215\n",
      "Batch: 22 / 125 Loss: 6.7364783\n",
      "Batch: 23 / 125 Loss: 6.669857\n",
      "Batch: 24 / 125 Loss: 6.2073064\n",
      "Batch: 25 / 125 Loss: 6.457866\n",
      "Batch: 26 / 125 Loss: 7.5057054\n",
      "Batch: 27 / 125 Loss: 6.391574\n",
      "Batch: 28 / 125 Loss: 7.881289\n",
      "Batch: 29 / 125 Loss: 7.2241197\n",
      "Batch: 30 / 125 Loss: 7.0190487\n",
      "Batch: 31 / 125 Loss: 7.4925537\n",
      "Batch: 32 / 125 Loss: 7.3852973\n",
      "Batch: 33 / 125 Loss: 6.792189\n",
      "Batch: 34 / 125 Loss: 6.66164\n",
      "Batch: 35 / 125 Loss: 6.8022\n",
      "Batch: 36 / 125 Loss: 6.5006895\n",
      "Batch: 37 / 125 Loss: 6.862532\n",
      "Batch: 38 / 125 Loss: 6.7248063\n",
      "Batch: 39 / 125 Loss: 6.7152715\n",
      "Batch: 40 / 125 Loss: 6.780916\n",
      "Batch: 41 / 125 Loss: 7.1129785\n",
      "Batch: 42 / 125 Loss: 6.864506\n",
      "Batch: 43 / 125 Loss: 7.612179\n",
      "Batch: 44 / 125 Loss: 7.4401402\n",
      "Batch: 45 / 125 Loss: 7.107547\n",
      "Batch: 46 / 125 Loss: 6.14897\n",
      "Batch: 47 / 125 Loss: 6.609885\n",
      "Batch: 48 / 125 Loss: 6.4679103\n",
      "Batch: 49 / 125 Loss: 6.651842\n",
      "Batch: 50 / 125 Loss: 5.991154\n",
      "Batch: 51 / 125 Loss: 5.762645\n",
      "Batch: 52 / 125 Loss: 8.105137\n",
      "Batch: 53 / 125 Loss: 7.178615\n",
      "Batch: 54 / 125 Loss: 6.598203\n",
      "Batch: 55 / 125 Loss: 7.206198\n",
      "Batch: 56 / 125 Loss: 6.801102\n",
      "Batch: 57 / 125 Loss: 6.387423\n",
      "Batch: 58 / 125 Loss: 7.5255513\n",
      "Batch: 59 / 125 Loss: 6.3756456\n",
      "Batch: 60 / 125 Loss: 6.9061584\n",
      "Batch: 61 / 125 Loss: 5.975542\n",
      "Batch: 62 / 125 Loss: 7.2466393\n",
      "Batch: 63 / 125 Loss: 6.4815216\n",
      "Batch: 64 / 125 Loss: 6.729968\n",
      "Batch: 65 / 125 Loss: 6.591121\n",
      "Batch: 66 / 125 Loss: 6.3737288\n",
      "Batch: 67 / 125 Loss: 6.4129906\n",
      "Batch: 68 / 125 Loss: 6.5704675\n",
      "Batch: 69 / 125 Loss: 6.9006395\n",
      "Batch: 70 / 125 Loss: 6.4167705\n",
      "Batch: 71 / 125 Loss: 5.9696183\n",
      "Batch: 72 / 125 Loss: 7.193785\n",
      "Batch: 73 / 125 Loss: 6.6454377\n",
      "Batch: 74 / 125 Loss: 6.6082916\n",
      "Batch: 75 / 125 Loss: 6.1398573\n",
      "Batch: 76 / 125 Loss: 6.113114\n",
      "Batch: 77 / 125 Loss: 5.697239\n",
      "Batch: 78 / 125 Loss: 6.815807\n",
      "Batch: 79 / 125 Loss: 7.5557623\n",
      "Batch: 80 / 125 Loss: 5.9699035\n",
      "Batch: 81 / 125 Loss: 5.991014\n",
      "Batch: 82 / 125 Loss: 5.789235\n",
      "Batch: 83 / 125 Loss: 5.2755666\n",
      "Batch: 84 / 125 Loss: 6.707964\n",
      "Batch: 85 / 125 Loss: 6.9081383\n",
      "Batch: 86 / 125 Loss: 6.6628323\n",
      "Batch: 87 / 125 Loss: 6.0623684\n",
      "Batch: 88 / 125 Loss: 6.3540344\n",
      "Batch: 89 / 125 Loss: 5.744787\n",
      "Batch: 90 / 125 Loss: 6.2553706\n",
      "Batch: 91 / 125 Loss: 6.106149\n",
      "Batch: 92 / 125 Loss: 6.405049\n",
      "Batch: 93 / 125 Loss: 6.090927\n",
      "Batch: 94 / 125 Loss: 6.367322\n",
      "Batch: 95 / 125 Loss: 6.160398\n",
      "Batch: 96 / 125 Loss: 6.0803947\n",
      "Batch: 97 / 125 Loss: 5.4380445\n",
      "Batch: 98 / 125 Loss: 5.6567574\n",
      "Batch: 99 / 125 Loss: 5.942281\n",
      "Batch: 100 / 125 Loss: 5.377057\n",
      "Batch: 101 / 125 Loss: 6.2794104\n",
      "Batch: 102 / 125 Loss: 6.937647\n",
      "Batch: 103 / 125 Loss: 6.677989\n",
      "Batch: 104 / 125 Loss: 6.4156857\n",
      "Batch: 105 / 125 Loss: 5.468026\n",
      "Batch: 106 / 125 Loss: 6.56122\n",
      "Batch: 107 / 125 Loss: 6.723383\n",
      "Batch: 108 / 125 Loss: 5.240703\n",
      "Batch: 109 / 125 Loss: 6.115012\n",
      "Batch: 110 / 125 Loss: 5.3175325\n",
      "Batch: 111 / 125 Loss: 6.098975\n",
      "Batch: 112 / 125 Loss: 6.2820325\n",
      "Batch: 113 / 125 Loss: 6.2769413\n",
      "Batch: 114 / 125 Loss: 6.8162475\n",
      "Batch: 115 / 125 Loss: 6.2029324\n",
      "Batch: 116 / 125 Loss: 5.9466767\n",
      "Batch: 117 / 125 Loss: 5.908642\n",
      "Batch: 118 / 125 Loss: 6.3585057\n",
      "Batch: 119 / 125 Loss: 5.5278697\n",
      "Batch: 120 / 125 Loss: 5.9206243\n",
      "Batch: 121 / 125 Loss: 6.1484675\n",
      "Batch: 122 / 125 Loss: 5.832029\n",
      "Batch: 123 / 125 Loss: 5.546537\n",
      "Batch: 124 / 125 Loss: 4.978952\n",
      "Batch: 125 / 125 Loss: 5.985253\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 45.339061%. Word accuracy: 31.196429%.\n",
      "character error rate : 0.453390605008\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 6\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 5.7068815\n",
      "Batch: 2 / 125 Loss: 5.588458\n",
      "Batch: 3 / 125 Loss: 5.9503627\n",
      "Batch: 4 / 125 Loss: 5.8960896\n",
      "Batch: 5 / 125 Loss: 6.7079444\n",
      "Batch: 6 / 125 Loss: 6.9718757\n",
      "Batch: 7 / 125 Loss: 5.3898077\n",
      "Batch: 8 / 125 Loss: 5.149765\n",
      "Batch: 9 / 125 Loss: 5.777746\n",
      "Batch: 10 / 125 Loss: 5.1288385\n",
      "Batch: 11 / 125 Loss: 6.0820704\n",
      "Batch: 12 / 125 Loss: 5.7662745\n",
      "Batch: 13 / 125 Loss: 5.57518\n",
      "Batch: 14 / 125 Loss: 5.098006\n",
      "Batch: 15 / 125 Loss: 5.2318907\n",
      "Batch: 16 / 125 Loss: 6.2815995\n",
      "Batch: 17 / 125 Loss: 6.1731277\n",
      "Batch: 18 / 125 Loss: 6.3673205\n",
      "Batch: 19 / 125 Loss: 5.2887406\n",
      "Batch: 20 / 125 Loss: 6.053534\n",
      "Batch: 21 / 125 Loss: 5.7728014\n",
      "Batch: 22 / 125 Loss: 5.5367165\n",
      "Batch: 23 / 125 Loss: 5.3129516\n",
      "Batch: 24 / 125 Loss: 5.597107\n",
      "Batch: 25 / 125 Loss: 6.3150845\n",
      "Batch: 26 / 125 Loss: 6.2178345\n",
      "Batch: 27 / 125 Loss: 5.443893\n",
      "Batch: 28 / 125 Loss: 5.445727\n",
      "Batch: 29 / 125 Loss: 4.943021\n",
      "Batch: 30 / 125 Loss: 5.701868\n",
      "Batch: 31 / 125 Loss: 6.5683665\n",
      "Batch: 32 / 125 Loss: 6.0035996\n",
      "Batch: 33 / 125 Loss: 5.307888\n",
      "Batch: 34 / 125 Loss: 5.4430566\n",
      "Batch: 35 / 125 Loss: 6.0204053\n",
      "Batch: 36 / 125 Loss: 5.0691743\n",
      "Batch: 37 / 125 Loss: 5.693896\n",
      "Batch: 38 / 125 Loss: 4.8862743\n",
      "Batch: 39 / 125 Loss: 5.5044374\n",
      "Batch: 40 / 125 Loss: 5.985038\n",
      "Batch: 41 / 125 Loss: 5.015483\n",
      "Batch: 42 / 125 Loss: 5.6910286\n",
      "Batch: 43 / 125 Loss: 5.2741766\n",
      "Batch: 44 / 125 Loss: 5.5749946\n",
      "Batch: 45 / 125 Loss: 5.568809\n",
      "Batch: 46 / 125 Loss: 5.0987024\n",
      "Batch: 47 / 125 Loss: 5.8594165\n",
      "Batch: 48 / 125 Loss: 5.650267\n",
      "Batch: 49 / 125 Loss: 5.47915\n",
      "Batch: 50 / 125 Loss: 6.0447254\n",
      "Batch: 51 / 125 Loss: 5.2860384\n",
      "Batch: 52 / 125 Loss: 5.995819\n",
      "Batch: 53 / 125 Loss: 5.5947204\n",
      "Batch: 54 / 125 Loss: 5.8522015\n",
      "Batch: 55 / 125 Loss: 5.6689324\n",
      "Batch: 56 / 125 Loss: 5.236353\n",
      "Batch: 57 / 125 Loss: 5.2649345\n",
      "Batch: 58 / 125 Loss: 5.3235188\n",
      "Batch: 59 / 125 Loss: 5.894843\n",
      "Batch: 60 / 125 Loss: 6.9953437\n",
      "Batch: 61 / 125 Loss: 6.491946\n",
      "Batch: 62 / 125 Loss: 5.965011\n",
      "Batch: 63 / 125 Loss: 5.6482263\n",
      "Batch: 64 / 125 Loss: 5.495812\n",
      "Batch: 65 / 125 Loss: 5.445797\n",
      "Batch: 66 / 125 Loss: 5.063653\n",
      "Batch: 67 / 125 Loss: 5.2330904\n",
      "Batch: 68 / 125 Loss: 6.7950244\n",
      "Batch: 69 / 125 Loss: 6.1745276\n",
      "Batch: 70 / 125 Loss: 4.9664474\n",
      "Batch: 71 / 125 Loss: 5.4719453\n",
      "Batch: 72 / 125 Loss: 5.390025\n",
      "Batch: 73 / 125 Loss: 5.1757174\n",
      "Batch: 74 / 125 Loss: 4.5294476\n",
      "Batch: 75 / 125 Loss: 6.1900563\n",
      "Batch: 76 / 125 Loss: 5.1523314\n",
      "Batch: 77 / 125 Loss: 6.38048\n",
      "Batch: 78 / 125 Loss: 4.941359\n",
      "Batch: 79 / 125 Loss: 5.146935\n",
      "Batch: 80 / 125 Loss: 5.1791053\n",
      "Batch: 81 / 125 Loss: 6.22185\n",
      "Batch: 82 / 125 Loss: 5.4012895\n",
      "Batch: 83 / 125 Loss: 5.1142387\n",
      "Batch: 84 / 125 Loss: 4.92283\n",
      "Batch: 85 / 125 Loss: 5.122141\n",
      "Batch: 86 / 125 Loss: 4.9051447\n",
      "Batch: 87 / 125 Loss: 5.657188\n",
      "Batch: 88 / 125 Loss: 5.7243176\n",
      "Batch: 89 / 125 Loss: 4.363428\n",
      "Batch: 90 / 125 Loss: 4.4816966\n",
      "Batch: 91 / 125 Loss: 5.2540836\n",
      "Batch: 92 / 125 Loss: 4.80706\n",
      "Batch: 93 / 125 Loss: 4.4678555\n",
      "Batch: 94 / 125 Loss: 5.2047925\n",
      "Batch: 95 / 125 Loss: 5.6627517\n",
      "Batch: 96 / 125 Loss: 4.763597\n",
      "Batch: 97 / 125 Loss: 4.678896\n",
      "Batch: 98 / 125 Loss: 4.7400517\n",
      "Batch: 99 / 125 Loss: 5.8290195\n",
      "Batch: 100 / 125 Loss: 5.876459\n",
      "Batch: 101 / 125 Loss: 5.5075073\n",
      "Batch: 102 / 125 Loss: 4.756821\n",
      "Batch: 103 / 125 Loss: 5.0577736\n",
      "Batch: 104 / 125 Loss: 5.301428\n",
      "Batch: 105 / 125 Loss: 5.2288356\n",
      "Batch: 106 / 125 Loss: 4.514345\n",
      "Batch: 107 / 125 Loss: 5.155179\n",
      "Batch: 108 / 125 Loss: 5.2718306\n",
      "Batch: 109 / 125 Loss: 4.985002\n",
      "Batch: 110 / 125 Loss: 5.275868\n",
      "Batch: 111 / 125 Loss: 4.6932487\n",
      "Batch: 112 / 125 Loss: 5.1862226\n",
      "Batch: 113 / 125 Loss: 5.063041\n",
      "Batch: 114 / 125 Loss: 5.35851\n",
      "Batch: 115 / 125 Loss: 4.9445744\n",
      "Batch: 116 / 125 Loss: 4.4532733\n",
      "Batch: 117 / 125 Loss: 5.4502144\n",
      "Batch: 118 / 125 Loss: 5.6399345\n",
      "Batch: 119 / 125 Loss: 5.439836\n",
      "Batch: 120 / 125 Loss: 5.2996483\n",
      "Batch: 121 / 125 Loss: 5.742358\n",
      "Batch: 122 / 125 Loss: 4.8364387\n",
      "Batch: 123 / 125 Loss: 4.863877\n",
      "Batch: 124 / 125 Loss: 5.1379023\n",
      "Batch: 125 / 125 Loss: 4.6085763\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 35.912082%. Word accuracy: 38.107143%.\n",
      "character error rate : 0.359120818863\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 7\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 4.829888\n",
      "Batch: 2 / 125 Loss: 5.3805604\n",
      "Batch: 3 / 125 Loss: 5.4900684\n",
      "Batch: 4 / 125 Loss: 5.2500677\n",
      "Batch: 5 / 125 Loss: 5.0396147\n",
      "Batch: 6 / 125 Loss: 5.084467\n",
      "Batch: 7 / 125 Loss: 5.499629\n",
      "Batch: 8 / 125 Loss: 4.996337\n",
      "Batch: 9 / 125 Loss: 5.5806203\n",
      "Batch: 10 / 125 Loss: 4.6861153\n",
      "Batch: 11 / 125 Loss: 5.488039\n",
      "Batch: 12 / 125 Loss: 4.5519567\n",
      "Batch: 13 / 125 Loss: 5.192167\n",
      "Batch: 14 / 125 Loss: 5.420861\n",
      "Batch: 15 / 125 Loss: 4.5715775\n",
      "Batch: 16 / 125 Loss: 5.4152207\n",
      "Batch: 17 / 125 Loss: 4.9430065\n",
      "Batch: 18 / 125 Loss: 5.3141856\n",
      "Batch: 19 / 125 Loss: 4.382193\n",
      "Batch: 20 / 125 Loss: 4.921989\n",
      "Batch: 21 / 125 Loss: 5.2525764\n",
      "Batch: 22 / 125 Loss: 4.391292\n",
      "Batch: 23 / 125 Loss: 4.331483\n",
      "Batch: 24 / 125 Loss: 4.9206214\n",
      "Batch: 25 / 125 Loss: 5.4263873\n",
      "Batch: 26 / 125 Loss: 4.3758683\n",
      "Batch: 27 / 125 Loss: 3.9182882\n",
      "Batch: 28 / 125 Loss: 5.6362677\n",
      "Batch: 29 / 125 Loss: 4.5681906\n",
      "Batch: 30 / 125 Loss: 5.3566966\n",
      "Batch: 31 / 125 Loss: 5.2089233\n",
      "Batch: 32 / 125 Loss: 4.065954\n",
      "Batch: 33 / 125 Loss: 5.8200808\n",
      "Batch: 34 / 125 Loss: 4.340744\n",
      "Batch: 35 / 125 Loss: 4.895805\n",
      "Batch: 36 / 125 Loss: 5.739173\n",
      "Batch: 37 / 125 Loss: 5.738152\n",
      "Batch: 38 / 125 Loss: 5.205304\n",
      "Batch: 39 / 125 Loss: 4.802527\n",
      "Batch: 40 / 125 Loss: 4.7776456\n",
      "Batch: 41 / 125 Loss: 5.600407\n",
      "Batch: 42 / 125 Loss: 4.2300234\n",
      "Batch: 43 / 125 Loss: 4.0817885\n",
      "Batch: 44 / 125 Loss: 4.590839\n",
      "Batch: 45 / 125 Loss: 5.389154\n",
      "Batch: 46 / 125 Loss: 5.879624\n",
      "Batch: 47 / 125 Loss: 5.08216\n",
      "Batch: 48 / 125 Loss: 5.158696\n",
      "Batch: 49 / 125 Loss: 4.6179533\n",
      "Batch: 50 / 125 Loss: 4.9628196\n",
      "Batch: 51 / 125 Loss: 5.197266\n",
      "Batch: 52 / 125 Loss: 4.561872\n",
      "Batch: 53 / 125 Loss: 5.2006254\n",
      "Batch: 54 / 125 Loss: 4.956885\n",
      "Batch: 55 / 125 Loss: 4.910699\n",
      "Batch: 56 / 125 Loss: 5.6011095\n",
      "Batch: 57 / 125 Loss: 4.9098816\n",
      "Batch: 58 / 125 Loss: 4.8784957\n",
      "Batch: 59 / 125 Loss: 4.8870726\n",
      "Batch: 60 / 125 Loss: 4.9598927\n",
      "Batch: 61 / 125 Loss: 4.4937825\n",
      "Batch: 62 / 125 Loss: 4.1903477\n",
      "Batch: 63 / 125 Loss: 4.57796\n",
      "Batch: 64 / 125 Loss: 4.5686398\n",
      "Batch: 65 / 125 Loss: 4.625556\n",
      "Batch: 66 / 125 Loss: 5.209329\n",
      "Batch: 67 / 125 Loss: 4.006894\n",
      "Batch: 68 / 125 Loss: 4.623434\n",
      "Batch: 69 / 125 Loss: 4.717898\n",
      "Batch: 70 / 125 Loss: 4.4590526\n",
      "Batch: 71 / 125 Loss: 5.569017\n",
      "Batch: 72 / 125 Loss: 5.005069\n",
      "Batch: 73 / 125 Loss: 4.395207\n",
      "Batch: 74 / 125 Loss: 5.1376886\n",
      "Batch: 75 / 125 Loss: 4.714721\n",
      "Batch: 76 / 125 Loss: 5.090588\n",
      "Batch: 77 / 125 Loss: 4.38303\n",
      "Batch: 78 / 125 Loss: 4.6867123\n",
      "Batch: 79 / 125 Loss: 4.4549346\n",
      "Batch: 80 / 125 Loss: 4.761551\n",
      "Batch: 81 / 125 Loss: 4.677209\n",
      "Batch: 82 / 125 Loss: 4.7324476\n",
      "Batch: 83 / 125 Loss: 3.9865348\n",
      "Batch: 84 / 125 Loss: 4.256744\n",
      "Batch: 85 / 125 Loss: 4.552723\n",
      "Batch: 86 / 125 Loss: 4.6542177\n",
      "Batch: 87 / 125 Loss: 3.9893909\n",
      "Batch: 88 / 125 Loss: 4.7981114\n",
      "Batch: 89 / 125 Loss: 4.539266\n",
      "Batch: 90 / 125 Loss: 4.696575\n",
      "Batch: 91 / 125 Loss: 5.114858\n",
      "Batch: 92 / 125 Loss: 5.154874\n",
      "Batch: 93 / 125 Loss: 4.6923738\n",
      "Batch: 94 / 125 Loss: 4.5534825\n",
      "Batch: 95 / 125 Loss: 4.4159117\n",
      "Batch: 96 / 125 Loss: 4.7779346\n",
      "Batch: 97 / 125 Loss: 4.1857657\n",
      "Batch: 98 / 125 Loss: 4.9594927\n",
      "Batch: 99 / 125 Loss: 4.790247\n",
      "Batch: 100 / 125 Loss: 4.7231183\n",
      "Batch: 101 / 125 Loss: 5.285322\n",
      "Batch: 102 / 125 Loss: 4.9400277\n",
      "Batch: 103 / 125 Loss: 4.754556\n",
      "Batch: 104 / 125 Loss: 4.2037945\n",
      "Batch: 105 / 125 Loss: 4.203189\n",
      "Batch: 106 / 125 Loss: 5.8681383\n",
      "Batch: 107 / 125 Loss: 4.1497173\n",
      "Batch: 108 / 125 Loss: 4.120681\n",
      "Batch: 109 / 125 Loss: 4.4022856\n",
      "Batch: 110 / 125 Loss: 4.969848\n",
      "Batch: 111 / 125 Loss: 5.6509156\n",
      "Batch: 112 / 125 Loss: 4.963091\n",
      "Batch: 113 / 125 Loss: 4.1273236\n",
      "Batch: 114 / 125 Loss: 4.3989797\n",
      "Batch: 115 / 125 Loss: 4.4253926\n",
      "Batch: 116 / 125 Loss: 3.890214\n",
      "Batch: 117 / 125 Loss: 5.0341372\n",
      "Batch: 118 / 125 Loss: 4.0222306\n",
      "Batch: 119 / 125 Loss: 5.254917\n",
      "Batch: 120 / 125 Loss: 4.288902\n",
      "Batch: 121 / 125 Loss: 5.03442\n",
      "Batch: 122 / 125 Loss: 4.6294804\n",
      "Batch: 123 / 125 Loss: 4.8335147\n",
      "Batch: 124 / 125 Loss: 3.859026\n",
      "Batch: 125 / 125 Loss: 4.1154656\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 37.488576%. Word accuracy: 35.089286%.\n",
      "character error rate : 0.374885761287\n",
      "Character error rate not improved\n",
      "Epoch:============================================================== 8\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 4.5357246\n",
      "Batch: 2 / 125 Loss: 4.457401\n",
      "Batch: 3 / 125 Loss: 4.91281\n",
      "Batch: 4 / 125 Loss: 4.7733173\n",
      "Batch: 5 / 125 Loss: 4.8581395\n",
      "Batch: 6 / 125 Loss: 4.2589693\n",
      "Batch: 7 / 125 Loss: 4.727124\n",
      "Batch: 8 / 125 Loss: 4.365564\n",
      "Batch: 9 / 125 Loss: 4.4709597\n",
      "Batch: 10 / 125 Loss: 4.26034\n",
      "Batch: 11 / 125 Loss: 4.481914\n",
      "Batch: 12 / 125 Loss: 4.060398\n",
      "Batch: 13 / 125 Loss: 4.2995596\n",
      "Batch: 14 / 125 Loss: 4.1519637\n",
      "Batch: 15 / 125 Loss: 4.145164\n",
      "Batch: 16 / 125 Loss: 4.36944\n",
      "Batch: 17 / 125 Loss: 4.2600503\n",
      "Batch: 18 / 125 Loss: 4.911824\n",
      "Batch: 19 / 125 Loss: 5.0670147\n",
      "Batch: 20 / 125 Loss: 4.9646873\n",
      "Batch: 21 / 125 Loss: 4.239652\n",
      "Batch: 22 / 125 Loss: 4.0922484\n",
      "Batch: 23 / 125 Loss: 3.914836\n",
      "Batch: 24 / 125 Loss: 4.3721356\n",
      "Batch: 25 / 125 Loss: 4.135641\n",
      "Batch: 26 / 125 Loss: 4.9296327\n",
      "Batch: 27 / 125 Loss: 3.9489746\n",
      "Batch: 28 / 125 Loss: 4.561271\n",
      "Batch: 29 / 125 Loss: 5.0094547\n",
      "Batch: 30 / 125 Loss: 4.14568\n",
      "Batch: 31 / 125 Loss: 4.399101\n",
      "Batch: 32 / 125 Loss: 4.468274\n",
      "Batch: 33 / 125 Loss: 4.909573\n",
      "Batch: 34 / 125 Loss: 4.4670625\n",
      "Batch: 35 / 125 Loss: 4.271683\n",
      "Batch: 36 / 125 Loss: 3.3729434\n",
      "Batch: 37 / 125 Loss: 4.673482\n",
      "Batch: 38 / 125 Loss: 4.1611567\n",
      "Batch: 39 / 125 Loss: 4.131575\n",
      "Batch: 40 / 125 Loss: 3.781882\n",
      "Batch: 41 / 125 Loss: 3.9835742\n",
      "Batch: 42 / 125 Loss: 4.320719\n",
      "Batch: 43 / 125 Loss: 4.3999996\n",
      "Batch: 44 / 125 Loss: 5.0931206\n",
      "Batch: 45 / 125 Loss: 4.0171456\n",
      "Batch: 46 / 125 Loss: 4.4031086\n",
      "Batch: 47 / 125 Loss: 3.9174497\n",
      "Batch: 48 / 125 Loss: 4.597745\n",
      "Batch: 49 / 125 Loss: 5.1025066\n",
      "Batch: 50 / 125 Loss: 4.395582\n",
      "Batch: 51 / 125 Loss: 4.011475\n",
      "Batch: 52 / 125 Loss: 4.690794\n",
      "Batch: 53 / 125 Loss: 4.116016\n",
      "Batch: 54 / 125 Loss: 3.9720602\n",
      "Batch: 55 / 125 Loss: 4.100183\n",
      "Batch: 56 / 125 Loss: 4.417594\n",
      "Batch: 57 / 125 Loss: 3.9695802\n",
      "Batch: 58 / 125 Loss: 3.6248279\n",
      "Batch: 59 / 125 Loss: 4.0364947\n",
      "Batch: 60 / 125 Loss: 3.7271695\n",
      "Batch: 61 / 125 Loss: 3.7691228\n",
      "Batch: 62 / 125 Loss: 4.97472\n",
      "Batch: 63 / 125 Loss: 3.9573472\n",
      "Batch: 64 / 125 Loss: 4.4387283\n",
      "Batch: 65 / 125 Loss: 3.951887\n",
      "Batch: 66 / 125 Loss: 4.452031\n",
      "Batch: 67 / 125 Loss: 4.8592587\n",
      "Batch: 68 / 125 Loss: 3.6908712\n",
      "Batch: 69 / 125 Loss: 3.59576\n",
      "Batch: 70 / 125 Loss: 4.185052\n",
      "Batch: 71 / 125 Loss: 4.4530473\n",
      "Batch: 72 / 125 Loss: 4.0125675\n",
      "Batch: 73 / 125 Loss: 3.8912756\n",
      "Batch: 74 / 125 Loss: 4.296441\n",
      "Batch: 75 / 125 Loss: 4.291434\n",
      "Batch: 76 / 125 Loss: 4.718757\n",
      "Batch: 77 / 125 Loss: 4.0246654\n",
      "Batch: 78 / 125 Loss: 4.5881405\n",
      "Batch: 79 / 125 Loss: 4.76095\n",
      "Batch: 80 / 125 Loss: 4.1131577\n",
      "Batch: 81 / 125 Loss: 4.504362\n",
      "Batch: 82 / 125 Loss: 4.3209257\n",
      "Batch: 83 / 125 Loss: 3.5005355\n",
      "Batch: 84 / 125 Loss: 4.1142073\n",
      "Batch: 85 / 125 Loss: 4.551631\n",
      "Batch: 86 / 125 Loss: 3.6751792\n",
      "Batch: 87 / 125 Loss: 3.6262095\n",
      "Batch: 88 / 125 Loss: 4.185411\n",
      "Batch: 89 / 125 Loss: 4.904198\n",
      "Batch: 90 / 125 Loss: 3.9107964\n",
      "Batch: 91 / 125 Loss: 4.4671574\n",
      "Batch: 92 / 125 Loss: 3.878029\n",
      "Batch: 93 / 125 Loss: 4.133819\n",
      "Batch: 94 / 125 Loss: 3.7445116\n",
      "Batch: 95 / 125 Loss: 3.7586088\n",
      "Batch: 96 / 125 Loss: 3.5225363\n",
      "Batch: 97 / 125 Loss: 4.3039894\n",
      "Batch: 98 / 125 Loss: 4.394397\n",
      "Batch: 99 / 125 Loss: 3.4012072\n",
      "Batch: 100 / 125 Loss: 4.61656\n",
      "Batch: 101 / 125 Loss: 3.8947327\n",
      "Batch: 102 / 125 Loss: 4.2092304\n",
      "Batch: 103 / 125 Loss: 4.718935\n",
      "Batch: 104 / 125 Loss: 4.612355\n",
      "Batch: 105 / 125 Loss: 4.318274\n",
      "Batch: 106 / 125 Loss: 4.233696\n",
      "Batch: 107 / 125 Loss: 4.075875\n",
      "Batch: 108 / 125 Loss: 4.4525623\n",
      "Batch: 109 / 125 Loss: 3.689975\n",
      "Batch: 110 / 125 Loss: 4.714707\n",
      "Batch: 111 / 125 Loss: 4.32241\n",
      "Batch: 112 / 125 Loss: 4.930574\n",
      "Batch: 113 / 125 Loss: 4.6733327\n",
      "Batch: 114 / 125 Loss: 3.641639\n",
      "Batch: 115 / 125 Loss: 4.1737175\n",
      "Batch: 116 / 125 Loss: 3.83906\n",
      "Batch: 117 / 125 Loss: 4.0465527\n",
      "Batch: 118 / 125 Loss: 4.7825975\n",
      "Batch: 119 / 125 Loss: 3.5520658\n",
      "Batch: 120 / 125 Loss: 4.635948\n",
      "Batch: 121 / 125 Loss: 4.855368\n",
      "Batch: 122 / 125 Loss: 4.27711\n",
      "Batch: 123 / 125 Loss: 4.014066\n",
      "Batch: 124 / 125 Loss: 4.254481\n",
      "Batch: 125 / 125 Loss: 3.9163308\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 26.763846%. Word accuracy: 47.910714%.\n",
      "character error rate : 0.26763845732\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 9\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 3.53692\n",
      "Batch: 2 / 125 Loss: 3.4661863\n",
      "Batch: 3 / 125 Loss: 4.299363\n",
      "Batch: 4 / 125 Loss: 4.433106\n",
      "Batch: 5 / 125 Loss: 4.222537\n",
      "Batch: 6 / 125 Loss: 3.8267531\n",
      "Batch: 7 / 125 Loss: 3.9211915\n",
      "Batch: 8 / 125 Loss: 4.4583373\n",
      "Batch: 9 / 125 Loss: 3.9640286\n",
      "Batch: 10 / 125 Loss: 4.0964665\n",
      "Batch: 11 / 125 Loss: 4.8120947\n",
      "Batch: 12 / 125 Loss: 4.3837004\n",
      "Batch: 13 / 125 Loss: 3.4360862\n",
      "Batch: 14 / 125 Loss: 4.2228417\n",
      "Batch: 15 / 125 Loss: 3.761562\n",
      "Batch: 16 / 125 Loss: 4.073124\n",
      "Batch: 17 / 125 Loss: 3.9242861\n",
      "Batch: 18 / 125 Loss: 3.2280471\n",
      "Batch: 19 / 125 Loss: 3.5823553\n",
      "Batch: 20 / 125 Loss: 4.5741115\n",
      "Batch: 21 / 125 Loss: 3.5446334\n",
      "Batch: 22 / 125 Loss: 3.7687604\n",
      "Batch: 23 / 125 Loss: 3.3023899\n",
      "Batch: 24 / 125 Loss: 3.1951761\n",
      "Batch: 25 / 125 Loss: 3.6470401\n",
      "Batch: 26 / 125 Loss: 4.252474\n",
      "Batch: 27 / 125 Loss: 3.7855167\n",
      "Batch: 28 / 125 Loss: 4.637661\n",
      "Batch: 29 / 125 Loss: 3.5235\n",
      "Batch: 30 / 125 Loss: 3.6762571\n",
      "Batch: 31 / 125 Loss: 4.1369944\n",
      "Batch: 32 / 125 Loss: 4.0993695\n",
      "Batch: 33 / 125 Loss: 4.223119\n",
      "Batch: 34 / 125 Loss: 3.9204328\n",
      "Batch: 35 / 125 Loss: 3.6540403\n",
      "Batch: 36 / 125 Loss: 4.5069137\n",
      "Batch: 37 / 125 Loss: 4.17466\n",
      "Batch: 38 / 125 Loss: 4.3500524\n",
      "Batch: 39 / 125 Loss: 4.550758\n",
      "Batch: 40 / 125 Loss: 3.7162933\n",
      "Batch: 41 / 125 Loss: 3.8009825\n",
      "Batch: 42 / 125 Loss: 3.2622027\n",
      "Batch: 43 / 125 Loss: 3.6754255\n",
      "Batch: 44 / 125 Loss: 3.5582845\n",
      "Batch: 45 / 125 Loss: 3.2999573\n",
      "Batch: 46 / 125 Loss: 3.7721655\n",
      "Batch: 47 / 125 Loss: 3.6742578\n",
      "Batch: 48 / 125 Loss: 4.185785\n",
      "Batch: 49 / 125 Loss: 4.0281606\n",
      "Batch: 50 / 125 Loss: 3.987173\n",
      "Batch: 51 / 125 Loss: 4.4221854\n",
      "Batch: 52 / 125 Loss: 4.3067517\n",
      "Batch: 53 / 125 Loss: 4.2432814\n",
      "Batch: 54 / 125 Loss: 3.8343287\n",
      "Batch: 55 / 125 Loss: 3.8208528\n",
      "Batch: 56 / 125 Loss: 4.2746778\n",
      "Batch: 57 / 125 Loss: 3.75227\n",
      "Batch: 58 / 125 Loss: 3.7447677\n",
      "Batch: 59 / 125 Loss: 3.1848722\n",
      "Batch: 60 / 125 Loss: 3.3909779\n",
      "Batch: 61 / 125 Loss: 4.330676\n",
      "Batch: 62 / 125 Loss: 4.391583\n",
      "Batch: 63 / 125 Loss: 3.9527366\n",
      "Batch: 64 / 125 Loss: 3.9020731\n",
      "Batch: 65 / 125 Loss: 3.6851652\n",
      "Batch: 66 / 125 Loss: 3.7688153\n",
      "Batch: 67 / 125 Loss: 4.513417\n",
      "Batch: 68 / 125 Loss: 4.2580733\n",
      "Batch: 69 / 125 Loss: 3.8694596\n",
      "Batch: 70 / 125 Loss: 4.4863834\n",
      "Batch: 71 / 125 Loss: 3.327627\n",
      "Batch: 72 / 125 Loss: 3.5616813\n",
      "Batch: 73 / 125 Loss: 3.952303\n",
      "Batch: 74 / 125 Loss: 3.7332184\n",
      "Batch: 75 / 125 Loss: 3.393197\n",
      "Batch: 76 / 125 Loss: 3.7612545\n",
      "Batch: 77 / 125 Loss: 4.6213655\n",
      "Batch: 78 / 125 Loss: 4.4517794\n",
      "Batch: 79 / 125 Loss: 3.7722542\n",
      "Batch: 80 / 125 Loss: 3.4181213\n",
      "Batch: 81 / 125 Loss: 4.0385904\n",
      "Batch: 82 / 125 Loss: 3.4977999\n",
      "Batch: 83 / 125 Loss: 3.1760316\n",
      "Batch: 84 / 125 Loss: 3.3343573\n",
      "Batch: 85 / 125 Loss: 3.518851\n",
      "Batch: 86 / 125 Loss: 3.3104663\n",
      "Batch: 87 / 125 Loss: 3.7222548\n",
      "Batch: 88 / 125 Loss: 4.285778\n",
      "Batch: 89 / 125 Loss: 3.7542717\n",
      "Batch: 90 / 125 Loss: 4.071453\n",
      "Batch: 91 / 125 Loss: 3.818001\n",
      "Batch: 92 / 125 Loss: 3.832299\n",
      "Batch: 93 / 125 Loss: 3.9481416\n",
      "Batch: 94 / 125 Loss: 3.5425556\n",
      "Batch: 95 / 125 Loss: 4.0879984\n",
      "Batch: 96 / 125 Loss: 3.4040868\n",
      "Batch: 97 / 125 Loss: 3.3618102\n",
      "Batch: 98 / 125 Loss: 3.3647277\n",
      "Batch: 99 / 125 Loss: 4.3409677\n",
      "Batch: 100 / 125 Loss: 4.0042276\n",
      "Batch: 101 / 125 Loss: 3.1258075\n",
      "Batch: 102 / 125 Loss: 3.9585614\n",
      "Batch: 103 / 125 Loss: 3.7331567\n",
      "Batch: 104 / 125 Loss: 3.718136\n",
      "Batch: 105 / 125 Loss: 4.105896\n",
      "Batch: 106 / 125 Loss: 3.1453726\n",
      "Batch: 107 / 125 Loss: 3.6400464\n",
      "Batch: 108 / 125 Loss: 3.8671856\n",
      "Batch: 109 / 125 Loss: 3.393479\n",
      "Batch: 110 / 125 Loss: 3.612753\n",
      "Batch: 111 / 125 Loss: 4.65034\n",
      "Batch: 112 / 125 Loss: 4.800179\n",
      "Batch: 113 / 125 Loss: 3.8605032\n",
      "Batch: 114 / 125 Loss: 4.0175123\n",
      "Batch: 115 / 125 Loss: 4.4015965\n",
      "Batch: 116 / 125 Loss: 4.3010635\n",
      "Batch: 117 / 125 Loss: 3.9783664\n",
      "Batch: 118 / 125 Loss: 3.386018\n",
      "Batch: 119 / 125 Loss: 4.051931\n",
      "Batch: 120 / 125 Loss: 3.8601043\n",
      "Batch: 121 / 125 Loss: 3.8309472\n",
      "Batch: 122 / 125 Loss: 3.7452538\n",
      "Batch: 123 / 125 Loss: 3.4587543\n",
      "Batch: 124 / 125 Loss: 4.0173864\n",
      "Batch: 125 / 125 Loss: 3.6558607\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 25.159934%. Word accuracy: 50.000000%.\n",
      "character error rate : 0.251599341985\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 10\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 4.0478797\n",
      "Batch: 2 / 125 Loss: 3.9294796\n",
      "Batch: 3 / 125 Loss: 3.1392646\n",
      "Batch: 4 / 125 Loss: 4.136672\n",
      "Batch: 5 / 125 Loss: 3.712367\n",
      "Batch: 6 / 125 Loss: 3.9654474\n",
      "Batch: 7 / 125 Loss: 3.254851\n",
      "Batch: 8 / 125 Loss: 4.2507854\n",
      "Batch: 9 / 125 Loss: 4.25152\n",
      "Batch: 10 / 125 Loss: 3.7422173\n",
      "Batch: 11 / 125 Loss: 3.6065042\n",
      "Batch: 12 / 125 Loss: 3.1986477\n",
      "Batch: 13 / 125 Loss: 3.0590396\n",
      "Batch: 14 / 125 Loss: 3.8242195\n",
      "Batch: 15 / 125 Loss: 2.957914\n",
      "Batch: 16 / 125 Loss: 4.6515617\n",
      "Batch: 17 / 125 Loss: 3.3497846\n",
      "Batch: 18 / 125 Loss: 3.8016121\n",
      "Batch: 19 / 125 Loss: 4.102713\n",
      "Batch: 20 / 125 Loss: 2.8159986\n",
      "Batch: 21 / 125 Loss: 4.3361406\n",
      "Batch: 22 / 125 Loss: 4.330471\n",
      "Batch: 23 / 125 Loss: 4.4269614\n",
      "Batch: 24 / 125 Loss: 3.6971874\n",
      "Batch: 25 / 125 Loss: 3.839958\n",
      "Batch: 26 / 125 Loss: 3.0100417\n",
      "Batch: 27 / 125 Loss: 2.9251275\n",
      "Batch: 28 / 125 Loss: 4.348554\n",
      "Batch: 29 / 125 Loss: 3.5111666\n",
      "Batch: 30 / 125 Loss: 3.6322346\n",
      "Batch: 31 / 125 Loss: 3.7850752\n",
      "Batch: 32 / 125 Loss: 3.9626563\n",
      "Batch: 33 / 125 Loss: 3.5337327\n",
      "Batch: 34 / 125 Loss: 3.1519353\n",
      "Batch: 35 / 125 Loss: 3.2298913\n",
      "Batch: 36 / 125 Loss: 3.8197541\n",
      "Batch: 37 / 125 Loss: 3.4627411\n",
      "Batch: 38 / 125 Loss: 4.023816\n",
      "Batch: 39 / 125 Loss: 3.510956\n",
      "Batch: 40 / 125 Loss: 4.0649586\n",
      "Batch: 41 / 125 Loss: 3.6147623\n",
      "Batch: 42 / 125 Loss: 3.7617962\n",
      "Batch: 43 / 125 Loss: 4.3648686\n",
      "Batch: 44 / 125 Loss: 3.4650378\n",
      "Batch: 45 / 125 Loss: 3.801687\n",
      "Batch: 46 / 125 Loss: 3.838535\n",
      "Batch: 47 / 125 Loss: 4.0118265\n",
      "Batch: 48 / 125 Loss: 3.547158\n",
      "Batch: 49 / 125 Loss: 3.1236753\n",
      "Batch: 50 / 125 Loss: 4.079953\n",
      "Batch: 51 / 125 Loss: 3.404799\n",
      "Batch: 52 / 125 Loss: 3.5586326\n",
      "Batch: 53 / 125 Loss: 3.8575716\n",
      "Batch: 54 / 125 Loss: 3.2918923\n",
      "Batch: 55 / 125 Loss: 2.9532068\n",
      "Batch: 56 / 125 Loss: 3.2906036\n",
      "Batch: 57 / 125 Loss: 3.2627869\n",
      "Batch: 58 / 125 Loss: 3.6082804\n",
      "Batch: 59 / 125 Loss: 2.7671747\n",
      "Batch: 60 / 125 Loss: 3.7434483\n",
      "Batch: 61 / 125 Loss: 3.3416338\n",
      "Batch: 62 / 125 Loss: 3.7533638\n",
      "Batch: 63 / 125 Loss: 3.1545064\n",
      "Batch: 64 / 125 Loss: 3.9111695\n",
      "Batch: 65 / 125 Loss: 3.497608\n",
      "Batch: 66 / 125 Loss: 3.7665148\n",
      "Batch: 67 / 125 Loss: 4.513727\n",
      "Batch: 68 / 125 Loss: 3.9213269\n",
      "Batch: 69 / 125 Loss: 3.6775897\n",
      "Batch: 70 / 125 Loss: 3.4147031\n",
      "Batch: 71 / 125 Loss: 3.3028283\n",
      "Batch: 72 / 125 Loss: 3.2799876\n",
      "Batch: 73 / 125 Loss: 3.3509703\n",
      "Batch: 74 / 125 Loss: 3.0924141\n",
      "Batch: 75 / 125 Loss: 2.892597\n",
      "Batch: 76 / 125 Loss: 3.948486\n",
      "Batch: 77 / 125 Loss: 3.5034604\n",
      "Batch: 78 / 125 Loss: 3.6377447\n",
      "Batch: 79 / 125 Loss: 3.2743115\n",
      "Batch: 80 / 125 Loss: 3.6171784\n",
      "Batch: 81 / 125 Loss: 3.5185077\n",
      "Batch: 82 / 125 Loss: 4.2022934\n",
      "Batch: 83 / 125 Loss: 4.4686937\n",
      "Batch: 84 / 125 Loss: 3.223614\n",
      "Batch: 85 / 125 Loss: 3.030776\n",
      "Batch: 86 / 125 Loss: 3.2864704\n",
      "Batch: 87 / 125 Loss: 3.0137026\n",
      "Batch: 88 / 125 Loss: 3.3620355\n",
      "Batch: 89 / 125 Loss: 3.180812\n",
      "Batch: 90 / 125 Loss: 3.7696896\n",
      "Batch: 91 / 125 Loss: 3.818889\n",
      "Batch: 92 / 125 Loss: 3.4242206\n",
      "Batch: 93 / 125 Loss: 3.819435\n",
      "Batch: 94 / 125 Loss: 3.2885842\n",
      "Batch: 95 / 125 Loss: 3.3164818\n",
      "Batch: 96 / 125 Loss: 3.3575857\n",
      "Batch: 97 / 125 Loss: 4.0964055\n",
      "Batch: 98 / 125 Loss: 3.7069557\n",
      "Batch: 99 / 125 Loss: 3.0882914\n",
      "Batch: 100 / 125 Loss: 3.5060523\n",
      "Batch: 101 / 125 Loss: 3.1654093\n",
      "Batch: 102 / 125 Loss: 3.5705786\n",
      "Batch: 103 / 125 Loss: 3.6838975\n",
      "Batch: 104 / 125 Loss: 2.7918289\n",
      "Batch: 105 / 125 Loss: 3.595179\n",
      "Batch: 106 / 125 Loss: 3.423937\n",
      "Batch: 107 / 125 Loss: 3.935601\n",
      "Batch: 108 / 125 Loss: 3.276211\n",
      "Batch: 109 / 125 Loss: 3.1463532\n",
      "Batch: 110 / 125 Loss: 3.0866034\n",
      "Batch: 111 / 125 Loss: 3.9180148\n",
      "Batch: 112 / 125 Loss: 3.3377895\n",
      "Batch: 113 / 125 Loss: 3.129048\n",
      "Batch: 114 / 125 Loss: 3.4204705\n",
      "Batch: 115 / 125 Loss: 3.0686731\n",
      "Batch: 116 / 125 Loss: 3.1093204\n",
      "Batch: 117 / 125 Loss: 3.8307505\n",
      "Batch: 118 / 125 Loss: 3.2791781\n",
      "Batch: 119 / 125 Loss: 3.1189902\n",
      "Batch: 120 / 125 Loss: 2.9989505\n",
      "Batch: 121 / 125 Loss: 2.7336078\n",
      "Batch: 122 / 125 Loss: 3.7886147\n",
      "Batch: 123 / 125 Loss: 3.2349486\n",
      "Batch: 124 / 125 Loss: 3.33042\n",
      "Batch: 125 / 125 Loss: 2.9885573\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 28.285505%. Word accuracy: 47.571429%.\n",
      "character error rate : 0.282855053921\n",
      "Character error rate not improved\n",
      "Epoch:============================================================== 11\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.9649017\n",
      "Batch: 2 / 125 Loss: 2.84789\n",
      "Batch: 3 / 125 Loss: 3.522409\n",
      "Batch: 4 / 125 Loss: 3.8441958\n",
      "Batch: 5 / 125 Loss: 3.6227045\n",
      "Batch: 6 / 125 Loss: 2.8809867\n",
      "Batch: 7 / 125 Loss: 3.510203\n",
      "Batch: 8 / 125 Loss: 3.8746276\n",
      "Batch: 9 / 125 Loss: 4.3563213\n",
      "Batch: 10 / 125 Loss: 3.7475147\n",
      "Batch: 11 / 125 Loss: 3.1497955\n",
      "Batch: 12 / 125 Loss: 2.8514245\n",
      "Batch: 13 / 125 Loss: 4.1077905\n",
      "Batch: 14 / 125 Loss: 3.6301348\n",
      "Batch: 15 / 125 Loss: 3.4417377\n",
      "Batch: 16 / 125 Loss: 3.1305244\n",
      "Batch: 17 / 125 Loss: 3.5821862\n",
      "Batch: 18 / 125 Loss: 3.7259986\n",
      "Batch: 19 / 125 Loss: 4.395695\n",
      "Batch: 20 / 125 Loss: 3.5275397\n",
      "Batch: 21 / 125 Loss: 3.3897488\n",
      "Batch: 22 / 125 Loss: 3.789671\n",
      "Batch: 23 / 125 Loss: 4.222635\n",
      "Batch: 24 / 125 Loss: 3.5549152\n",
      "Batch: 25 / 125 Loss: 2.680053\n",
      "Batch: 26 / 125 Loss: 2.6784046\n",
      "Batch: 27 / 125 Loss: 3.0401254\n",
      "Batch: 28 / 125 Loss: 2.834335\n",
      "Batch: 29 / 125 Loss: 3.6115158\n",
      "Batch: 30 / 125 Loss: 3.6345322\n",
      "Batch: 31 / 125 Loss: 3.323518\n",
      "Batch: 32 / 125 Loss: 3.8306916\n",
      "Batch: 33 / 125 Loss: 3.6400592\n",
      "Batch: 34 / 125 Loss: 3.619207\n",
      "Batch: 35 / 125 Loss: 3.2916913\n",
      "Batch: 36 / 125 Loss: 3.183878\n",
      "Batch: 37 / 125 Loss: 3.5503376\n",
      "Batch: 38 / 125 Loss: 3.4310367\n",
      "Batch: 39 / 125 Loss: 3.5706627\n",
      "Batch: 40 / 125 Loss: 3.0177536\n",
      "Batch: 41 / 125 Loss: 3.063771\n",
      "Batch: 42 / 125 Loss: 2.9927838\n",
      "Batch: 43 / 125 Loss: 3.8655417\n",
      "Batch: 44 / 125 Loss: 3.5382907\n",
      "Batch: 45 / 125 Loss: 3.1996174\n",
      "Batch: 46 / 125 Loss: 3.3657699\n",
      "Batch: 47 / 125 Loss: 3.2942588\n",
      "Batch: 48 / 125 Loss: 3.4539392\n",
      "Batch: 49 / 125 Loss: 2.7237482\n",
      "Batch: 50 / 125 Loss: 3.3073194\n",
      "Batch: 51 / 125 Loss: 3.162531\n",
      "Batch: 52 / 125 Loss: 3.5720196\n",
      "Batch: 53 / 125 Loss: 2.981223\n",
      "Batch: 54 / 125 Loss: 3.392469\n",
      "Batch: 55 / 125 Loss: 3.554299\n",
      "Batch: 56 / 125 Loss: 4.3207126\n",
      "Batch: 57 / 125 Loss: 3.4002979\n",
      "Batch: 58 / 125 Loss: 3.8573928\n",
      "Batch: 59 / 125 Loss: 3.3158634\n",
      "Batch: 60 / 125 Loss: 3.166224\n",
      "Batch: 61 / 125 Loss: 3.4704947\n",
      "Batch: 62 / 125 Loss: 3.208061\n",
      "Batch: 63 / 125 Loss: 3.160963\n",
      "Batch: 64 / 125 Loss: 3.7500732\n",
      "Batch: 65 / 125 Loss: 3.3493063\n",
      "Batch: 66 / 125 Loss: 3.9821882\n",
      "Batch: 67 / 125 Loss: 3.5995142\n",
      "Batch: 68 / 125 Loss: 3.7369256\n",
      "Batch: 69 / 125 Loss: 4.243585\n",
      "Batch: 70 / 125 Loss: 3.697439\n",
      "Batch: 71 / 125 Loss: 3.6342378\n",
      "Batch: 72 / 125 Loss: 3.3212647\n",
      "Batch: 73 / 125 Loss: 3.1924512\n",
      "Batch: 74 / 125 Loss: 3.933068\n",
      "Batch: 75 / 125 Loss: 3.3676302\n",
      "Batch: 76 / 125 Loss: 3.3581216\n",
      "Batch: 77 / 125 Loss: 3.1300528\n",
      "Batch: 78 / 125 Loss: 3.3412163\n",
      "Batch: 79 / 125 Loss: 3.112132\n",
      "Batch: 80 / 125 Loss: 2.3163357\n",
      "Batch: 81 / 125 Loss: 3.388881\n",
      "Batch: 82 / 125 Loss: 3.5002158\n",
      "Batch: 83 / 125 Loss: 3.0777438\n",
      "Batch: 84 / 125 Loss: 3.2491846\n",
      "Batch: 85 / 125 Loss: 3.5205152\n",
      "Batch: 86 / 125 Loss: 3.8004203\n",
      "Batch: 87 / 125 Loss: 3.7746644\n",
      "Batch: 88 / 125 Loss: 4.1137204\n",
      "Batch: 89 / 125 Loss: 3.566126\n",
      "Batch: 90 / 125 Loss: 3.5022094\n",
      "Batch: 91 / 125 Loss: 4.100382\n",
      "Batch: 92 / 125 Loss: 4.349069\n",
      "Batch: 93 / 125 Loss: 3.5946648\n",
      "Batch: 94 / 125 Loss: 3.867981\n",
      "Batch: 95 / 125 Loss: 3.8373513\n",
      "Batch: 96 / 125 Loss: 3.519419\n",
      "Batch: 97 / 125 Loss: 3.1534786\n",
      "Batch: 98 / 125 Loss: 3.001203\n",
      "Batch: 99 / 125 Loss: 3.8447595\n",
      "Batch: 100 / 125 Loss: 3.0561852\n",
      "Batch: 101 / 125 Loss: 3.9330664\n",
      "Batch: 102 / 125 Loss: 3.392282\n",
      "Batch: 103 / 125 Loss: 3.4676456\n",
      "Batch: 104 / 125 Loss: 3.5403638\n",
      "Batch: 105 / 125 Loss: 3.749649\n",
      "Batch: 106 / 125 Loss: 3.4889047\n",
      "Batch: 107 / 125 Loss: 3.0834482\n",
      "Batch: 108 / 125 Loss: 3.4565547\n",
      "Batch: 109 / 125 Loss: 3.1063774\n",
      "Batch: 110 / 125 Loss: 3.8023603\n",
      "Batch: 111 / 125 Loss: 2.6898305\n",
      "Batch: 112 / 125 Loss: 2.853002\n",
      "Batch: 113 / 125 Loss: 3.5161223\n",
      "Batch: 114 / 125 Loss: 3.0372827\n",
      "Batch: 115 / 125 Loss: 3.2985559\n",
      "Batch: 116 / 125 Loss: 2.8676648\n",
      "Batch: 117 / 125 Loss: 3.825914\n",
      "Batch: 118 / 125 Loss: 3.4775324\n",
      "Batch: 119 / 125 Loss: 2.8343737\n",
      "Batch: 120 / 125 Loss: 3.3575394\n",
      "Batch: 121 / 125 Loss: 3.369986\n",
      "Batch: 122 / 125 Loss: 3.8552234\n",
      "Batch: 123 / 125 Loss: 3.2357812\n",
      "Batch: 124 / 125 Loss: 2.7052326\n",
      "Batch: 125 / 125 Loss: 3.2288766\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 23.560592%. Word accuracy: 52.589286%.\n",
      "character error rate : 0.235605922135\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 12\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 3.3114846\n",
      "Batch: 2 / 125 Loss: 2.8595243\n",
      "Batch: 3 / 125 Loss: 3.1145658\n",
      "Batch: 4 / 125 Loss: 3.0097497\n",
      "Batch: 5 / 125 Loss: 3.0789788\n",
      "Batch: 6 / 125 Loss: 3.245889\n",
      "Batch: 7 / 125 Loss: 3.238581\n",
      "Batch: 8 / 125 Loss: 3.5997987\n",
      "Batch: 9 / 125 Loss: 2.8901603\n",
      "Batch: 10 / 125 Loss: 3.9741256\n",
      "Batch: 11 / 125 Loss: 3.9122708\n",
      "Batch: 12 / 125 Loss: 2.7576654\n",
      "Batch: 13 / 125 Loss: 2.9803336\n",
      "Batch: 14 / 125 Loss: 3.3554206\n",
      "Batch: 15 / 125 Loss: 3.251791\n",
      "Batch: 16 / 125 Loss: 2.970737\n",
      "Batch: 17 / 125 Loss: 2.7186446\n",
      "Batch: 18 / 125 Loss: 3.7735941\n",
      "Batch: 19 / 125 Loss: 2.8199334\n",
      "Batch: 20 / 125 Loss: 3.6170545\n",
      "Batch: 21 / 125 Loss: 2.771393\n",
      "Batch: 22 / 125 Loss: 2.869747\n",
      "Batch: 23 / 125 Loss: 3.2077236\n",
      "Batch: 24 / 125 Loss: 3.9378512\n",
      "Batch: 25 / 125 Loss: 3.5949423\n",
      "Batch: 26 / 125 Loss: 2.868881\n",
      "Batch: 27 / 125 Loss: 2.6613064\n",
      "Batch: 28 / 125 Loss: 3.264957\n",
      "Batch: 29 / 125 Loss: 3.9517365\n",
      "Batch: 30 / 125 Loss: 3.6594882\n",
      "Batch: 31 / 125 Loss: 2.9045973\n",
      "Batch: 32 / 125 Loss: 3.9862885\n",
      "Batch: 33 / 125 Loss: 3.2586627\n",
      "Batch: 34 / 125 Loss: 2.9805403\n",
      "Batch: 35 / 125 Loss: 4.1825967\n",
      "Batch: 36 / 125 Loss: 4.0982847\n",
      "Batch: 37 / 125 Loss: 3.0905917\n",
      "Batch: 38 / 125 Loss: 3.1062164\n",
      "Batch: 39 / 125 Loss: 2.9240432\n",
      "Batch: 40 / 125 Loss: 4.0128922\n",
      "Batch: 41 / 125 Loss: 3.5496836\n",
      "Batch: 42 / 125 Loss: 2.8829832\n",
      "Batch: 43 / 125 Loss: 3.0268555\n",
      "Batch: 44 / 125 Loss: 2.5954847\n",
      "Batch: 45 / 125 Loss: 3.7116897\n",
      "Batch: 46 / 125 Loss: 3.3055387\n",
      "Batch: 47 / 125 Loss: 3.0834968\n",
      "Batch: 48 / 125 Loss: 2.9343405\n",
      "Batch: 49 / 125 Loss: 3.2605774\n",
      "Batch: 50 / 125 Loss: 3.735889\n",
      "Batch: 51 / 125 Loss: 2.5246925\n",
      "Batch: 52 / 125 Loss: 3.366263\n",
      "Batch: 53 / 125 Loss: 3.1771865\n",
      "Batch: 54 / 125 Loss: 3.5839808\n",
      "Batch: 55 / 125 Loss: 3.6429477\n",
      "Batch: 56 / 125 Loss: 3.638006\n",
      "Batch: 57 / 125 Loss: 3.373302\n",
      "Batch: 58 / 125 Loss: 3.5595233\n",
      "Batch: 59 / 125 Loss: 3.385933\n",
      "Batch: 60 / 125 Loss: 2.9891138\n",
      "Batch: 61 / 125 Loss: 2.9491189\n",
      "Batch: 62 / 125 Loss: 3.288256\n",
      "Batch: 63 / 125 Loss: 2.9576445\n",
      "Batch: 64 / 125 Loss: 3.6446598\n",
      "Batch: 65 / 125 Loss: 3.7155917\n",
      "Batch: 66 / 125 Loss: 3.1513104\n",
      "Batch: 67 / 125 Loss: 2.922701\n",
      "Batch: 68 / 125 Loss: 3.2361383\n",
      "Batch: 69 / 125 Loss: 4.029833\n",
      "Batch: 70 / 125 Loss: 4.0709453\n",
      "Batch: 71 / 125 Loss: 2.8438504\n",
      "Batch: 72 / 125 Loss: 3.3964877\n",
      "Batch: 73 / 125 Loss: 3.1682227\n",
      "Batch: 74 / 125 Loss: 2.9412615\n",
      "Batch: 75 / 125 Loss: 3.1180866\n",
      "Batch: 76 / 125 Loss: 3.376064\n",
      "Batch: 77 / 125 Loss: 2.672696\n",
      "Batch: 78 / 125 Loss: 4.2350383\n",
      "Batch: 79 / 125 Loss: 2.86289\n",
      "Batch: 80 / 125 Loss: 3.5929232\n",
      "Batch: 81 / 125 Loss: 3.0788467\n",
      "Batch: 82 / 125 Loss: 3.6268575\n",
      "Batch: 83 / 125 Loss: 3.3696854\n",
      "Batch: 84 / 125 Loss: 2.7433577\n",
      "Batch: 85 / 125 Loss: 2.6708722\n",
      "Batch: 86 / 125 Loss: 3.5204337\n",
      "Batch: 87 / 125 Loss: 3.5578997\n",
      "Batch: 88 / 125 Loss: 3.3234968\n",
      "Batch: 89 / 125 Loss: 3.000701\n",
      "Batch: 90 / 125 Loss: 2.9993613\n",
      "Batch: 91 / 125 Loss: 3.599112\n",
      "Batch: 92 / 125 Loss: 3.0889292\n",
      "Batch: 93 / 125 Loss: 2.7403684\n",
      "Batch: 94 / 125 Loss: 3.241195\n",
      "Batch: 95 / 125 Loss: 3.660302\n",
      "Batch: 96 / 125 Loss: 3.5484314\n",
      "Batch: 97 / 125 Loss: 2.7665973\n",
      "Batch: 98 / 125 Loss: 3.5776048\n",
      "Batch: 99 / 125 Loss: 3.4307537\n",
      "Batch: 100 / 125 Loss: 3.1846747\n",
      "Batch: 101 / 125 Loss: 3.4192476\n",
      "Batch: 102 / 125 Loss: 3.1307569\n",
      "Batch: 103 / 125 Loss: 4.200283\n",
      "Batch: 104 / 125 Loss: 3.3468432\n",
      "Batch: 105 / 125 Loss: 2.5769947\n",
      "Batch: 106 / 125 Loss: 2.9750948\n",
      "Batch: 107 / 125 Loss: 3.1153371\n",
      "Batch: 108 / 125 Loss: 3.786398\n",
      "Batch: 109 / 125 Loss: 2.9233422\n",
      "Batch: 110 / 125 Loss: 2.4668493\n",
      "Batch: 111 / 125 Loss: 2.7837393\n",
      "Batch: 112 / 125 Loss: 2.911874\n",
      "Batch: 113 / 125 Loss: 3.68293\n",
      "Batch: 114 / 125 Loss: 3.2015357\n",
      "Batch: 115 / 125 Loss: 3.6774676\n",
      "Batch: 116 / 125 Loss: 3.4230218\n",
      "Batch: 117 / 125 Loss: 3.0898025\n",
      "Batch: 118 / 125 Loss: 3.042999\n",
      "Batch: 119 / 125 Loss: 3.0632691\n",
      "Batch: 120 / 125 Loss: 2.886638\n",
      "Batch: 121 / 125 Loss: 3.2928648\n",
      "Batch: 122 / 125 Loss: 3.0607092\n",
      "Batch: 123 / 125 Loss: 3.5801315\n",
      "Batch: 124 / 125 Loss: 3.5731027\n",
      "Batch: 125 / 125 Loss: 3.050865\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 22.824895%. Word accuracy: 54.107143%.\n",
      "character error rate : 0.228248949004\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 13\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 3.4761746\n",
      "Batch: 2 / 125 Loss: 3.0690641\n",
      "Batch: 3 / 125 Loss: 2.976993\n",
      "Batch: 4 / 125 Loss: 2.6638248\n",
      "Batch: 5 / 125 Loss: 3.4548438\n",
      "Batch: 6 / 125 Loss: 2.844136\n",
      "Batch: 7 / 125 Loss: 3.8283057\n",
      "Batch: 8 / 125 Loss: 4.1760654\n",
      "Batch: 9 / 125 Loss: 2.9532783\n",
      "Batch: 10 / 125 Loss: 3.6748278\n",
      "Batch: 11 / 125 Loss: 3.3611224\n",
      "Batch: 12 / 125 Loss: 3.4932685\n",
      "Batch: 13 / 125 Loss: 3.5342774\n",
      "Batch: 14 / 125 Loss: 3.0265157\n",
      "Batch: 15 / 125 Loss: 2.6183496\n",
      "Batch: 16 / 125 Loss: 2.7921653\n",
      "Batch: 17 / 125 Loss: 2.9600275\n",
      "Batch: 18 / 125 Loss: 3.364232\n",
      "Batch: 19 / 125 Loss: 2.960965\n",
      "Batch: 20 / 125 Loss: 3.2735093\n",
      "Batch: 21 / 125 Loss: 2.809392\n",
      "Batch: 22 / 125 Loss: 2.7840743\n",
      "Batch: 23 / 125 Loss: 3.5435433\n",
      "Batch: 24 / 125 Loss: 3.594592\n",
      "Batch: 25 / 125 Loss: 3.0476098\n",
      "Batch: 26 / 125 Loss: 2.6697977\n",
      "Batch: 27 / 125 Loss: 3.4004898\n",
      "Batch: 28 / 125 Loss: 3.2398598\n",
      "Batch: 29 / 125 Loss: 3.4947844\n",
      "Batch: 30 / 125 Loss: 2.7348957\n",
      "Batch: 31 / 125 Loss: 3.237341\n",
      "Batch: 32 / 125 Loss: 2.7777538\n",
      "Batch: 33 / 125 Loss: 2.6349192\n",
      "Batch: 34 / 125 Loss: 3.7956495\n",
      "Batch: 35 / 125 Loss: 3.1748667\n",
      "Batch: 36 / 125 Loss: 3.1235974\n",
      "Batch: 37 / 125 Loss: 3.4112792\n",
      "Batch: 38 / 125 Loss: 2.7083294\n",
      "Batch: 39 / 125 Loss: 3.5184152\n",
      "Batch: 40 / 125 Loss: 3.2014923\n",
      "Batch: 41 / 125 Loss: 3.3106658\n",
      "Batch: 42 / 125 Loss: 3.5639744\n",
      "Batch: 43 / 125 Loss: 2.6306639\n",
      "Batch: 44 / 125 Loss: 3.4438367\n",
      "Batch: 45 / 125 Loss: 3.3752413\n",
      "Batch: 46 / 125 Loss: 2.933719\n",
      "Batch: 47 / 125 Loss: 2.8118863\n",
      "Batch: 48 / 125 Loss: 3.2093735\n",
      "Batch: 49 / 125 Loss: 2.7943745\n",
      "Batch: 50 / 125 Loss: 3.1173725\n",
      "Batch: 51 / 125 Loss: 3.143726\n",
      "Batch: 52 / 125 Loss: 2.5884628\n",
      "Batch: 53 / 125 Loss: 2.770603\n",
      "Batch: 54 / 125 Loss: 2.83771\n",
      "Batch: 55 / 125 Loss: 2.9755294\n",
      "Batch: 56 / 125 Loss: 3.108576\n",
      "Batch: 57 / 125 Loss: 2.7813241\n",
      "Batch: 58 / 125 Loss: 4.0516696\n",
      "Batch: 59 / 125 Loss: 2.7124348\n",
      "Batch: 60 / 125 Loss: 3.0088992\n",
      "Batch: 61 / 125 Loss: 2.4586954\n",
      "Batch: 62 / 125 Loss: 2.9909787\n",
      "Batch: 63 / 125 Loss: 3.063426\n",
      "Batch: 64 / 125 Loss: 3.236088\n",
      "Batch: 65 / 125 Loss: 2.7385929\n",
      "Batch: 66 / 125 Loss: 2.318568\n",
      "Batch: 67 / 125 Loss: 3.1609027\n",
      "Batch: 68 / 125 Loss: 2.841887\n",
      "Batch: 69 / 125 Loss: 2.3238375\n",
      "Batch: 70 / 125 Loss: 3.134052\n",
      "Batch: 71 / 125 Loss: 3.2002697\n",
      "Batch: 72 / 125 Loss: 3.1667423\n",
      "Batch: 73 / 125 Loss: 2.8618636\n",
      "Batch: 74 / 125 Loss: 3.4417605\n",
      "Batch: 75 / 125 Loss: 3.117111\n",
      "Batch: 76 / 125 Loss: 3.1370509\n",
      "Batch: 77 / 125 Loss: 2.8793364\n",
      "Batch: 78 / 125 Loss: 3.1069934\n",
      "Batch: 79 / 125 Loss: 3.4766593\n",
      "Batch: 80 / 125 Loss: 2.9196427\n",
      "Batch: 81 / 125 Loss: 2.881895\n",
      "Batch: 82 / 125 Loss: 2.7057116\n",
      "Batch: 83 / 125 Loss: 3.041033\n",
      "Batch: 84 / 125 Loss: 2.79012\n",
      "Batch: 85 / 125 Loss: 2.608161\n",
      "Batch: 86 / 125 Loss: 2.4361398\n",
      "Batch: 87 / 125 Loss: 3.1648505\n",
      "Batch: 88 / 125 Loss: 3.674382\n",
      "Batch: 89 / 125 Loss: 3.7552085\n",
      "Batch: 90 / 125 Loss: 2.5277762\n",
      "Batch: 91 / 125 Loss: 3.9218333\n",
      "Batch: 92 / 125 Loss: 2.9070334\n",
      "Batch: 93 / 125 Loss: 3.1215057\n",
      "Batch: 94 / 125 Loss: 2.5183065\n",
      "Batch: 95 / 125 Loss: 3.0187178\n",
      "Batch: 96 / 125 Loss: 2.5472162\n",
      "Batch: 97 / 125 Loss: 3.545994\n",
      "Batch: 98 / 125 Loss: 2.864874\n",
      "Batch: 99 / 125 Loss: 3.2524407\n",
      "Batch: 100 / 125 Loss: 3.1655626\n",
      "Batch: 101 / 125 Loss: 2.868977\n",
      "Batch: 102 / 125 Loss: 2.7317824\n",
      "Batch: 103 / 125 Loss: 3.391816\n",
      "Batch: 104 / 125 Loss: 3.4694343\n",
      "Batch: 105 / 125 Loss: 2.4536512\n",
      "Batch: 106 / 125 Loss: 2.900166\n",
      "Batch: 107 / 125 Loss: 2.3359818\n",
      "Batch: 108 / 125 Loss: 2.8886054\n",
      "Batch: 109 / 125 Loss: 2.7464025\n",
      "Batch: 110 / 125 Loss: 2.4768593\n",
      "Batch: 111 / 125 Loss: 2.8746161\n",
      "Batch: 112 / 125 Loss: 2.8128207\n",
      "Batch: 113 / 125 Loss: 2.3293982\n",
      "Batch: 114 / 125 Loss: 3.47134\n",
      "Batch: 115 / 125 Loss: 2.7155662\n",
      "Batch: 116 / 125 Loss: 2.866713\n",
      "Batch: 117 / 125 Loss: 2.9049811\n",
      "Batch: 118 / 125 Loss: 2.8866203\n",
      "Batch: 119 / 125 Loss: 2.8835282\n",
      "Batch: 120 / 125 Loss: 3.4410143\n",
      "Batch: 121 / 125 Loss: 2.6129086\n",
      "Batch: 122 / 125 Loss: 3.220358\n",
      "Batch: 123 / 125 Loss: 3.4833872\n",
      "Batch: 124 / 125 Loss: 3.5725315\n",
      "Batch: 125 / 125 Loss: 3.03406\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 21.796746%. Word accuracy: 54.589286%.\n",
      "character error rate : 0.217967464814\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 14\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.5692937\n",
      "Batch: 2 / 125 Loss: 2.7541718\n",
      "Batch: 3 / 125 Loss: 2.7785032\n",
      "Batch: 4 / 125 Loss: 2.713686\n",
      "Batch: 5 / 125 Loss: 3.0901055\n",
      "Batch: 6 / 125 Loss: 3.115148\n",
      "Batch: 7 / 125 Loss: 2.584449\n",
      "Batch: 8 / 125 Loss: 3.9221582\n",
      "Batch: 9 / 125 Loss: 3.44397\n",
      "Batch: 10 / 125 Loss: 3.1787012\n",
      "Batch: 11 / 125 Loss: 2.371457\n",
      "Batch: 12 / 125 Loss: 3.4623008\n",
      "Batch: 13 / 125 Loss: 3.0590057\n",
      "Batch: 14 / 125 Loss: 2.8753543\n",
      "Batch: 15 / 125 Loss: 2.7348688\n",
      "Batch: 16 / 125 Loss: 2.7012353\n",
      "Batch: 17 / 125 Loss: 3.5660105\n",
      "Batch: 18 / 125 Loss: 3.1110828\n",
      "Batch: 19 / 125 Loss: 2.792596\n",
      "Batch: 20 / 125 Loss: 3.627608\n",
      "Batch: 21 / 125 Loss: 2.7103918\n",
      "Batch: 22 / 125 Loss: 2.7754438\n",
      "Batch: 23 / 125 Loss: 2.8040087\n",
      "Batch: 24 / 125 Loss: 2.4886446\n",
      "Batch: 25 / 125 Loss: 3.3807697\n",
      "Batch: 26 / 125 Loss: 2.674685\n",
      "Batch: 27 / 125 Loss: 2.6434813\n",
      "Batch: 28 / 125 Loss: 2.958959\n",
      "Batch: 29 / 125 Loss: 2.7524335\n",
      "Batch: 30 / 125 Loss: 3.0403373\n",
      "Batch: 31 / 125 Loss: 2.7352462\n",
      "Batch: 32 / 125 Loss: 2.724435\n",
      "Batch: 33 / 125 Loss: 2.6403522\n",
      "Batch: 34 / 125 Loss: 3.1603172\n",
      "Batch: 35 / 125 Loss: 3.7282271\n",
      "Batch: 36 / 125 Loss: 3.019592\n",
      "Batch: 37 / 125 Loss: 2.7028472\n",
      "Batch: 38 / 125 Loss: 2.9312246\n",
      "Batch: 39 / 125 Loss: 2.7087903\n",
      "Batch: 40 / 125 Loss: 2.3337557\n",
      "Batch: 41 / 125 Loss: 2.6645734\n",
      "Batch: 42 / 125 Loss: 3.3550963\n",
      "Batch: 43 / 125 Loss: 2.854952\n",
      "Batch: 44 / 125 Loss: 2.9739764\n",
      "Batch: 45 / 125 Loss: 2.9426672\n",
      "Batch: 46 / 125 Loss: 2.7219622\n",
      "Batch: 47 / 125 Loss: 2.6055\n",
      "Batch: 48 / 125 Loss: 2.807879\n",
      "Batch: 49 / 125 Loss: 2.4289565\n",
      "Batch: 50 / 125 Loss: 3.189575\n",
      "Batch: 51 / 125 Loss: 2.6971228\n",
      "Batch: 52 / 125 Loss: 2.9027863\n",
      "Batch: 53 / 125 Loss: 2.628748\n",
      "Batch: 54 / 125 Loss: 2.5946064\n",
      "Batch: 55 / 125 Loss: 2.9398804\n",
      "Batch: 56 / 125 Loss: 2.4903543\n",
      "Batch: 57 / 125 Loss: 3.0579717\n",
      "Batch: 58 / 125 Loss: 2.4148257\n",
      "Batch: 59 / 125 Loss: 3.0373363\n",
      "Batch: 60 / 125 Loss: 3.0753896\n",
      "Batch: 61 / 125 Loss: 2.5689971\n",
      "Batch: 62 / 125 Loss: 2.9801471\n",
      "Batch: 63 / 125 Loss: 2.8038137\n",
      "Batch: 64 / 125 Loss: 2.3414917\n",
      "Batch: 65 / 125 Loss: 3.1401577\n",
      "Batch: 66 / 125 Loss: 2.8353639\n",
      "Batch: 67 / 125 Loss: 2.5927594\n",
      "Batch: 68 / 125 Loss: 3.3535922\n",
      "Batch: 69 / 125 Loss: 3.3880734\n",
      "Batch: 70 / 125 Loss: 2.6229556\n",
      "Batch: 71 / 125 Loss: 3.4610238\n",
      "Batch: 72 / 125 Loss: 3.6518826\n",
      "Batch: 73 / 125 Loss: 2.7053835\n",
      "Batch: 74 / 125 Loss: 2.6553006\n",
      "Batch: 75 / 125 Loss: 3.1404638\n",
      "Batch: 76 / 125 Loss: 2.8385596\n",
      "Batch: 77 / 125 Loss: 2.7107322\n",
      "Batch: 78 / 125 Loss: 2.8103597\n",
      "Batch: 79 / 125 Loss: 3.2457972\n",
      "Batch: 80 / 125 Loss: 2.9007893\n",
      "Batch: 81 / 125 Loss: 2.9393933\n",
      "Batch: 82 / 125 Loss: 2.8208735\n",
      "Batch: 83 / 125 Loss: 3.5538437\n",
      "Batch: 84 / 125 Loss: 3.2016778\n",
      "Batch: 85 / 125 Loss: 2.4256191\n",
      "Batch: 86 / 125 Loss: 2.33742\n",
      "Batch: 87 / 125 Loss: 3.233854\n",
      "Batch: 88 / 125 Loss: 2.7562792\n",
      "Batch: 89 / 125 Loss: 3.1217053\n",
      "Batch: 90 / 125 Loss: 2.6385682\n",
      "Batch: 91 / 125 Loss: 2.566771\n",
      "Batch: 92 / 125 Loss: 3.3032923\n",
      "Batch: 93 / 125 Loss: 2.9619324\n",
      "Batch: 94 / 125 Loss: 2.6133442\n",
      "Batch: 95 / 125 Loss: 2.7678432\n",
      "Batch: 96 / 125 Loss: 3.2347848\n",
      "Batch: 97 / 125 Loss: 2.5759876\n",
      "Batch: 98 / 125 Loss: 3.3227267\n",
      "Batch: 99 / 125 Loss: 2.751334\n",
      "Batch: 100 / 125 Loss: 2.7538574\n",
      "Batch: 101 / 125 Loss: 2.2744236\n",
      "Batch: 102 / 125 Loss: 2.2317867\n",
      "Batch: 103 / 125 Loss: 3.4322376\n",
      "Batch: 104 / 125 Loss: 3.0115812\n",
      "Batch: 105 / 125 Loss: 2.6598861\n",
      "Batch: 106 / 125 Loss: 2.7540607\n",
      "Batch: 107 / 125 Loss: 2.5151048\n",
      "Batch: 108 / 125 Loss: 2.8035576\n",
      "Batch: 109 / 125 Loss: 2.4761388\n",
      "Batch: 110 / 125 Loss: 3.0878167\n",
      "Batch: 111 / 125 Loss: 3.1940331\n",
      "Batch: 112 / 125 Loss: 2.4205728\n",
      "Batch: 113 / 125 Loss: 3.0661743\n",
      "Batch: 114 / 125 Loss: 2.6598268\n",
      "Batch: 115 / 125 Loss: 2.7710028\n",
      "Batch: 116 / 125 Loss: 2.7067218\n",
      "Batch: 117 / 125 Loss: 3.139676\n",
      "Batch: 118 / 125 Loss: 2.6288776\n",
      "Batch: 119 / 125 Loss: 2.6349597\n",
      "Batch: 120 / 125 Loss: 2.3950033\n",
      "Batch: 121 / 125 Loss: 2.9466147\n",
      "Batch: 122 / 125 Loss: 3.2116468\n",
      "Batch: 123 / 125 Loss: 2.7646203\n",
      "Batch: 124 / 125 Loss: 2.8439722\n",
      "Batch: 125 / 125 Loss: 3.6181393\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 24.483641%. Word accuracy: 50.125000%.\n",
      "character error rate : 0.244836410163\n",
      "Character error rate not improved\n",
      "Epoch:============================================================== 15\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.9485352\n",
      "Batch: 2 / 125 Loss: 2.8213336\n",
      "Batch: 3 / 125 Loss: 3.072903\n",
      "Batch: 4 / 125 Loss: 2.221685\n",
      "Batch: 5 / 125 Loss: 3.1521454\n",
      "Batch: 6 / 125 Loss: 2.6338265\n",
      "Batch: 7 / 125 Loss: 2.8950095\n",
      "Batch: 8 / 125 Loss: 2.743564\n",
      "Batch: 9 / 125 Loss: 2.3438127\n",
      "Batch: 10 / 125 Loss: 2.4497967\n",
      "Batch: 11 / 125 Loss: 2.4822843\n",
      "Batch: 12 / 125 Loss: 2.8754828\n",
      "Batch: 13 / 125 Loss: 2.9551644\n",
      "Batch: 14 / 125 Loss: 2.7041154\n",
      "Batch: 15 / 125 Loss: 2.5711136\n",
      "Batch: 16 / 125 Loss: 3.0687723\n",
      "Batch: 17 / 125 Loss: 2.8803947\n",
      "Batch: 18 / 125 Loss: 2.7667487\n",
      "Batch: 19 / 125 Loss: 2.388871\n",
      "Batch: 20 / 125 Loss: 2.5394883\n",
      "Batch: 21 / 125 Loss: 3.2835681\n",
      "Batch: 22 / 125 Loss: 3.302705\n",
      "Batch: 23 / 125 Loss: 3.0311582\n",
      "Batch: 24 / 125 Loss: 3.032716\n",
      "Batch: 25 / 125 Loss: 2.7916796\n",
      "Batch: 26 / 125 Loss: 2.4555542\n",
      "Batch: 27 / 125 Loss: 2.3066769\n",
      "Batch: 28 / 125 Loss: 3.129621\n",
      "Batch: 29 / 125 Loss: 2.8581488\n",
      "Batch: 30 / 125 Loss: 2.6350708\n",
      "Batch: 31 / 125 Loss: 2.8063428\n",
      "Batch: 32 / 125 Loss: 2.6003923\n",
      "Batch: 33 / 125 Loss: 3.0942457\n",
      "Batch: 34 / 125 Loss: 2.1946099\n",
      "Batch: 35 / 125 Loss: 3.2804263\n",
      "Batch: 36 / 125 Loss: 3.0448556\n",
      "Batch: 37 / 125 Loss: 2.902707\n",
      "Batch: 38 / 125 Loss: 2.85499\n",
      "Batch: 39 / 125 Loss: 2.639602\n",
      "Batch: 40 / 125 Loss: 2.4897208\n",
      "Batch: 41 / 125 Loss: 2.6578948\n",
      "Batch: 42 / 125 Loss: 2.5255468\n",
      "Batch: 43 / 125 Loss: 3.0779095\n",
      "Batch: 44 / 125 Loss: 2.6154966\n",
      "Batch: 45 / 125 Loss: 2.652481\n",
      "Batch: 46 / 125 Loss: 3.1534083\n",
      "Batch: 47 / 125 Loss: 3.1732788\n",
      "Batch: 48 / 125 Loss: 3.1097736\n",
      "Batch: 49 / 125 Loss: 3.2027051\n",
      "Batch: 50 / 125 Loss: 3.114115\n",
      "Batch: 51 / 125 Loss: 2.795315\n",
      "Batch: 52 / 125 Loss: 3.271616\n",
      "Batch: 53 / 125 Loss: 2.8268697\n",
      "Batch: 54 / 125 Loss: 3.0789573\n",
      "Batch: 55 / 125 Loss: 2.3529525\n",
      "Batch: 56 / 125 Loss: 3.6300921\n",
      "Batch: 57 / 125 Loss: 2.8729968\n",
      "Batch: 58 / 125 Loss: 2.4009774\n",
      "Batch: 59 / 125 Loss: 2.885122\n",
      "Batch: 60 / 125 Loss: 2.7487128\n",
      "Batch: 61 / 125 Loss: 2.4828382\n",
      "Batch: 62 / 125 Loss: 2.8040698\n",
      "Batch: 63 / 125 Loss: 3.1184893\n",
      "Batch: 64 / 125 Loss: 3.0537298\n",
      "Batch: 65 / 125 Loss: 2.5840292\n",
      "Batch: 66 / 125 Loss: 3.4192243\n",
      "Batch: 67 / 125 Loss: 2.82119\n",
      "Batch: 68 / 125 Loss: 3.0237353\n",
      "Batch: 69 / 125 Loss: 2.5134392\n",
      "Batch: 70 / 125 Loss: 2.815119\n",
      "Batch: 71 / 125 Loss: 2.8953888\n",
      "Batch: 72 / 125 Loss: 2.4758267\n",
      "Batch: 73 / 125 Loss: 2.7398849\n",
      "Batch: 74 / 125 Loss: 2.6612327\n",
      "Batch: 75 / 125 Loss: 3.1930423\n",
      "Batch: 76 / 125 Loss: 2.6525025\n",
      "Batch: 77 / 125 Loss: 3.134039\n",
      "Batch: 78 / 125 Loss: 3.7860239\n",
      "Batch: 79 / 125 Loss: 2.5987196\n",
      "Batch: 80 / 125 Loss: 2.5388556\n",
      "Batch: 81 / 125 Loss: 3.043291\n",
      "Batch: 82 / 125 Loss: 2.8821151\n",
      "Batch: 83 / 125 Loss: 3.0893192\n",
      "Batch: 84 / 125 Loss: 2.9122133\n",
      "Batch: 85 / 125 Loss: 2.748454\n",
      "Batch: 86 / 125 Loss: 2.615327\n",
      "Batch: 87 / 125 Loss: 2.9079292\n",
      "Batch: 88 / 125 Loss: 3.0210633\n",
      "Batch: 89 / 125 Loss: 2.5802457\n",
      "Batch: 90 / 125 Loss: 2.694752\n",
      "Batch: 91 / 125 Loss: 2.5449066\n",
      "Batch: 92 / 125 Loss: 2.656234\n",
      "Batch: 93 / 125 Loss: 2.536441\n",
      "Batch: 94 / 125 Loss: 2.2147245\n",
      "Batch: 95 / 125 Loss: 3.130783\n",
      "Batch: 96 / 125 Loss: 3.2948928\n",
      "Batch: 97 / 125 Loss: 2.9730642\n",
      "Batch: 98 / 125 Loss: 2.7543478\n",
      "Batch: 99 / 125 Loss: 3.2055478\n",
      "Batch: 100 / 125 Loss: 3.0676746\n",
      "Batch: 101 / 125 Loss: 2.417829\n",
      "Batch: 102 / 125 Loss: 2.5863192\n",
      "Batch: 103 / 125 Loss: 2.7855225\n",
      "Batch: 104 / 125 Loss: 3.4910195\n",
      "Batch: 105 / 125 Loss: 2.524956\n",
      "Batch: 106 / 125 Loss: 2.9703932\n",
      "Batch: 107 / 125 Loss: 3.2435718\n",
      "Batch: 108 / 125 Loss: 2.9117303\n",
      "Batch: 109 / 125 Loss: 2.897956\n",
      "Batch: 110 / 125 Loss: 3.4975817\n",
      "Batch: 111 / 125 Loss: 3.3336859\n",
      "Batch: 112 / 125 Loss: 3.406404\n",
      "Batch: 113 / 125 Loss: 2.4786828\n",
      "Batch: 114 / 125 Loss: 3.0937455\n",
      "Batch: 115 / 125 Loss: 2.6648514\n",
      "Batch: 116 / 125 Loss: 2.7056425\n",
      "Batch: 117 / 125 Loss: 2.582986\n",
      "Batch: 118 / 125 Loss: 3.0277417\n",
      "Batch: 119 / 125 Loss: 2.4735675\n",
      "Batch: 120 / 125 Loss: 3.0534155\n",
      "Batch: 121 / 125 Loss: 2.8202164\n",
      "Batch: 122 / 125 Loss: 2.8876226\n",
      "Batch: 123 / 125 Loss: 2.9262924\n",
      "Batch: 124 / 125 Loss: 2.9603193\n",
      "Batch: 125 / 125 Loss: 2.9868906\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 21.481448%. Word accuracy: 56.392857%.\n",
      "character error rate : 0.21481447633\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 16\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.2073224\n",
      "Batch: 2 / 125 Loss: 3.0556324\n",
      "Batch: 3 / 125 Loss: 3.0796387\n",
      "Batch: 4 / 125 Loss: 2.7060657\n",
      "Batch: 5 / 125 Loss: 2.4441612\n",
      "Batch: 6 / 125 Loss: 3.1507185\n",
      "Batch: 7 / 125 Loss: 2.8553295\n",
      "Batch: 8 / 125 Loss: 3.446031\n",
      "Batch: 9 / 125 Loss: 3.3178854\n",
      "Batch: 10 / 125 Loss: 3.04209\n",
      "Batch: 11 / 125 Loss: 2.6559696\n",
      "Batch: 12 / 125 Loss: 2.9869034\n",
      "Batch: 13 / 125 Loss: 3.4339302\n",
      "Batch: 14 / 125 Loss: 2.319131\n",
      "Batch: 15 / 125 Loss: 3.5642884\n",
      "Batch: 16 / 125 Loss: 3.067862\n",
      "Batch: 17 / 125 Loss: 3.022809\n",
      "Batch: 18 / 125 Loss: 2.985303\n",
      "Batch: 19 / 125 Loss: 2.54848\n",
      "Batch: 20 / 125 Loss: 2.7311437\n",
      "Batch: 21 / 125 Loss: 2.7811043\n",
      "Batch: 22 / 125 Loss: 2.5420294\n",
      "Batch: 23 / 125 Loss: 2.99328\n",
      "Batch: 24 / 125 Loss: 2.708985\n",
      "Batch: 25 / 125 Loss: 3.007518\n",
      "Batch: 26 / 125 Loss: 3.2173147\n",
      "Batch: 27 / 125 Loss: 2.0688229\n",
      "Batch: 28 / 125 Loss: 2.0186083\n",
      "Batch: 29 / 125 Loss: 2.5546749\n",
      "Batch: 30 / 125 Loss: 2.5920205\n",
      "Batch: 31 / 125 Loss: 2.600939\n",
      "Batch: 32 / 125 Loss: 2.5749977\n",
      "Batch: 33 / 125 Loss: 2.8424113\n",
      "Batch: 34 / 125 Loss: 2.9701385\n",
      "Batch: 35 / 125 Loss: 2.8890543\n",
      "Batch: 36 / 125 Loss: 3.1391435\n",
      "Batch: 37 / 125 Loss: 2.7585402\n",
      "Batch: 38 / 125 Loss: 2.4261863\n",
      "Batch: 39 / 125 Loss: 2.4694593\n",
      "Batch: 40 / 125 Loss: 2.450627\n",
      "Batch: 41 / 125 Loss: 2.6415615\n",
      "Batch: 42 / 125 Loss: 2.959066\n",
      "Batch: 43 / 125 Loss: 3.0740147\n",
      "Batch: 44 / 125 Loss: 2.6085486\n",
      "Batch: 45 / 125 Loss: 3.0433383\n",
      "Batch: 46 / 125 Loss: 3.1763182\n",
      "Batch: 47 / 125 Loss: 3.1776311\n",
      "Batch: 48 / 125 Loss: 2.7871168\n",
      "Batch: 49 / 125 Loss: 2.745176\n",
      "Batch: 50 / 125 Loss: 2.6360712\n",
      "Batch: 51 / 125 Loss: 2.6120944\n",
      "Batch: 52 / 125 Loss: 2.7916305\n",
      "Batch: 53 / 125 Loss: 3.0006766\n",
      "Batch: 54 / 125 Loss: 3.6177332\n",
      "Batch: 55 / 125 Loss: 2.9432251\n",
      "Batch: 56 / 125 Loss: 2.8091269\n",
      "Batch: 57 / 125 Loss: 2.2740319\n",
      "Batch: 58 / 125 Loss: 2.2533615\n",
      "Batch: 59 / 125 Loss: 2.2620203\n",
      "Batch: 60 / 125 Loss: 2.8517158\n",
      "Batch: 61 / 125 Loss: 2.7845788\n",
      "Batch: 62 / 125 Loss: 2.6522388\n",
      "Batch: 63 / 125 Loss: 3.1448343\n",
      "Batch: 64 / 125 Loss: 2.9433947\n",
      "Batch: 65 / 125 Loss: 2.7087831\n",
      "Batch: 66 / 125 Loss: 2.6505172\n",
      "Batch: 67 / 125 Loss: 3.764665\n",
      "Batch: 68 / 125 Loss: 2.6630735\n",
      "Batch: 69 / 125 Loss: 2.8514884\n",
      "Batch: 70 / 125 Loss: 2.0392463\n",
      "Batch: 71 / 125 Loss: 2.9632218\n",
      "Batch: 72 / 125 Loss: 3.2377942\n",
      "Batch: 73 / 125 Loss: 2.550587\n",
      "Batch: 74 / 125 Loss: 3.224482\n",
      "Batch: 75 / 125 Loss: 2.724952\n",
      "Batch: 76 / 125 Loss: 2.438436\n",
      "Batch: 77 / 125 Loss: 3.1042793\n",
      "Batch: 78 / 125 Loss: 3.2513134\n",
      "Batch: 79 / 125 Loss: 2.268872\n",
      "Batch: 80 / 125 Loss: 2.915642\n",
      "Batch: 81 / 125 Loss: 2.522181\n",
      "Batch: 82 / 125 Loss: 2.7183733\n",
      "Batch: 83 / 125 Loss: 2.618552\n",
      "Batch: 84 / 125 Loss: 2.4522617\n",
      "Batch: 85 / 125 Loss: 2.5174558\n",
      "Batch: 86 / 125 Loss: 2.3677669\n",
      "Batch: 87 / 125 Loss: 2.378171\n",
      "Batch: 88 / 125 Loss: 2.4028063\n",
      "Batch: 89 / 125 Loss: 2.866366\n",
      "Batch: 90 / 125 Loss: 3.1675494\n",
      "Batch: 91 / 125 Loss: 2.323676\n",
      "Batch: 92 / 125 Loss: 3.3405828\n",
      "Batch: 93 / 125 Loss: 2.6729903\n",
      "Batch: 94 / 125 Loss: 2.688106\n",
      "Batch: 95 / 125 Loss: 2.8614058\n",
      "Batch: 96 / 125 Loss: 2.509654\n",
      "Batch: 97 / 125 Loss: 2.487405\n",
      "Batch: 98 / 125 Loss: 2.3858576\n",
      "Batch: 99 / 125 Loss: 2.2552054\n",
      "Batch: 100 / 125 Loss: 2.6840515\n",
      "Batch: 101 / 125 Loss: 3.0373743\n",
      "Batch: 102 / 125 Loss: 2.732795\n",
      "Batch: 103 / 125 Loss: 3.0541666\n",
      "Batch: 104 / 125 Loss: 3.6117144\n",
      "Batch: 105 / 125 Loss: 2.8773553\n",
      "Batch: 106 / 125 Loss: 2.5869534\n",
      "Batch: 107 / 125 Loss: 2.1791441\n",
      "Batch: 108 / 125 Loss: 2.1955264\n",
      "Batch: 109 / 125 Loss: 3.0653808\n",
      "Batch: 110 / 125 Loss: 2.503106\n",
      "Batch: 111 / 125 Loss: 2.5732486\n",
      "Batch: 112 / 125 Loss: 2.576251\n",
      "Batch: 113 / 125 Loss: 2.3321333\n",
      "Batch: 114 / 125 Loss: 2.5547602\n",
      "Batch: 115 / 125 Loss: 3.740348\n",
      "Batch: 116 / 125 Loss: 2.7005045\n",
      "Batch: 117 / 125 Loss: 2.5415175\n",
      "Batch: 118 / 125 Loss: 2.3855238\n",
      "Batch: 119 / 125 Loss: 3.1678703\n",
      "Batch: 120 / 125 Loss: 2.3735032\n",
      "Batch: 121 / 125 Loss: 2.342801\n",
      "Batch: 122 / 125 Loss: 2.4477684\n",
      "Batch: 123 / 125 Loss: 2.087801\n",
      "Batch: 124 / 125 Loss: 2.544132\n",
      "Batch: 125 / 125 Loss: 2.665388\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 20.051179%. Word accuracy: 56.464286%.\n",
      "character error rate : 0.200511789435\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 17\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.8191135\n",
      "Batch: 2 / 125 Loss: 2.7226493\n",
      "Batch: 3 / 125 Loss: 2.4693818\n",
      "Batch: 4 / 125 Loss: 2.904965\n",
      "Batch: 5 / 125 Loss: 2.9105177\n",
      "Batch: 6 / 125 Loss: 2.6944664\n",
      "Batch: 7 / 125 Loss: 2.8604326\n",
      "Batch: 8 / 125 Loss: 1.9354995\n",
      "Batch: 9 / 125 Loss: 2.7771053\n",
      "Batch: 10 / 125 Loss: 2.4224114\n",
      "Batch: 11 / 125 Loss: 2.343821\n",
      "Batch: 12 / 125 Loss: 2.078927\n",
      "Batch: 13 / 125 Loss: 3.292644\n",
      "Batch: 14 / 125 Loss: 3.1441996\n",
      "Batch: 15 / 125 Loss: 2.6546497\n",
      "Batch: 16 / 125 Loss: 2.6156855\n",
      "Batch: 17 / 125 Loss: 2.901521\n",
      "Batch: 18 / 125 Loss: 2.812645\n",
      "Batch: 19 / 125 Loss: 2.5679824\n",
      "Batch: 20 / 125 Loss: 2.2791855\n",
      "Batch: 21 / 125 Loss: 2.0787508\n",
      "Batch: 22 / 125 Loss: 2.917498\n",
      "Batch: 23 / 125 Loss: 3.0445325\n",
      "Batch: 24 / 125 Loss: 2.6725593\n",
      "Batch: 25 / 125 Loss: 2.8652718\n",
      "Batch: 26 / 125 Loss: 2.4423406\n",
      "Batch: 27 / 125 Loss: 2.214309\n",
      "Batch: 28 / 125 Loss: 2.3783882\n",
      "Batch: 29 / 125 Loss: 2.1539323\n",
      "Batch: 30 / 125 Loss: 2.4886892\n",
      "Batch: 31 / 125 Loss: 3.0858505\n",
      "Batch: 32 / 125 Loss: 2.222294\n",
      "Batch: 33 / 125 Loss: 2.4425797\n",
      "Batch: 34 / 125 Loss: 2.846985\n",
      "Batch: 35 / 125 Loss: 3.4290204\n",
      "Batch: 36 / 125 Loss: 2.3140726\n",
      "Batch: 37 / 125 Loss: 2.770619\n",
      "Batch: 38 / 125 Loss: 3.024837\n",
      "Batch: 39 / 125 Loss: 2.8932834\n",
      "Batch: 40 / 125 Loss: 2.4942548\n",
      "Batch: 41 / 125 Loss: 3.5922332\n",
      "Batch: 42 / 125 Loss: 3.0265954\n",
      "Batch: 43 / 125 Loss: 3.5293832\n",
      "Batch: 44 / 125 Loss: 2.9556434\n",
      "Batch: 45 / 125 Loss: 2.8645966\n",
      "Batch: 46 / 125 Loss: 2.1986725\n",
      "Batch: 47 / 125 Loss: 2.8576324\n",
      "Batch: 48 / 125 Loss: 3.128698\n",
      "Batch: 49 / 125 Loss: 2.3904388\n",
      "Batch: 50 / 125 Loss: 3.0769105\n",
      "Batch: 51 / 125 Loss: 2.6565096\n",
      "Batch: 52 / 125 Loss: 3.1032119\n",
      "Batch: 53 / 125 Loss: 3.2545478\n",
      "Batch: 54 / 125 Loss: 2.1889448\n",
      "Batch: 55 / 125 Loss: 2.775714\n",
      "Batch: 56 / 125 Loss: 2.8395052\n",
      "Batch: 57 / 125 Loss: 2.4814355\n",
      "Batch: 58 / 125 Loss: 2.7035763\n",
      "Batch: 59 / 125 Loss: 2.7753015\n",
      "Batch: 60 / 125 Loss: 2.7861798\n",
      "Batch: 61 / 125 Loss: 2.382183\n",
      "Batch: 62 / 125 Loss: 2.9665897\n",
      "Batch: 63 / 125 Loss: 2.8222513\n",
      "Batch: 64 / 125 Loss: 2.252821\n",
      "Batch: 65 / 125 Loss: 2.509156\n",
      "Batch: 66 / 125 Loss: 2.0103188\n",
      "Batch: 67 / 125 Loss: 2.447525\n",
      "Batch: 68 / 125 Loss: 3.0318208\n",
      "Batch: 69 / 125 Loss: 2.9287252\n",
      "Batch: 70 / 125 Loss: 2.634444\n",
      "Batch: 71 / 125 Loss: 2.3623579\n",
      "Batch: 72 / 125 Loss: 2.5830142\n",
      "Batch: 73 / 125 Loss: 2.7271512\n",
      "Batch: 74 / 125 Loss: 2.31556\n",
      "Batch: 75 / 125 Loss: 2.4034657\n",
      "Batch: 76 / 125 Loss: 2.1743922\n",
      "Batch: 77 / 125 Loss: 1.8712147\n",
      "Batch: 78 / 125 Loss: 2.6735327\n",
      "Batch: 79 / 125 Loss: 2.8580291\n",
      "Batch: 80 / 125 Loss: 2.479644\n",
      "Batch: 81 / 125 Loss: 2.4263313\n",
      "Batch: 82 / 125 Loss: 2.6215458\n",
      "Batch: 83 / 125 Loss: 2.4636822\n",
      "Batch: 84 / 125 Loss: 2.5086324\n",
      "Batch: 85 / 125 Loss: 2.165296\n",
      "Batch: 86 / 125 Loss: 2.5911095\n",
      "Batch: 87 / 125 Loss: 2.4821503\n",
      "Batch: 88 / 125 Loss: 2.4705882\n",
      "Batch: 89 / 125 Loss: 2.4852314\n",
      "Batch: 90 / 125 Loss: 3.107117\n",
      "Batch: 91 / 125 Loss: 2.9565256\n",
      "Batch: 92 / 125 Loss: 2.4091322\n",
      "Batch: 93 / 125 Loss: 2.5699558\n",
      "Batch: 94 / 125 Loss: 2.6324644\n",
      "Batch: 95 / 125 Loss: 2.1838858\n",
      "Batch: 96 / 125 Loss: 2.5019722\n",
      "Batch: 97 / 125 Loss: 2.4320045\n",
      "Batch: 98 / 125 Loss: 2.6492019\n",
      "Batch: 99 / 125 Loss: 2.8992493\n",
      "Batch: 100 / 125 Loss: 3.4059076\n",
      "Batch: 101 / 125 Loss: 2.4377952\n",
      "Batch: 102 / 125 Loss: 2.6612618\n",
      "Batch: 103 / 125 Loss: 2.363392\n",
      "Batch: 104 / 125 Loss: 2.521362\n",
      "Batch: 105 / 125 Loss: 2.720445\n",
      "Batch: 106 / 125 Loss: 2.3792727\n",
      "Batch: 107 / 125 Loss: 2.6647656\n",
      "Batch: 108 / 125 Loss: 2.825393\n",
      "Batch: 109 / 125 Loss: 3.0958304\n",
      "Batch: 110 / 125 Loss: 2.8370576\n",
      "Batch: 111 / 125 Loss: 2.8876688\n",
      "Batch: 112 / 125 Loss: 2.4145355\n",
      "Batch: 113 / 125 Loss: 2.038707\n",
      "Batch: 114 / 125 Loss: 1.8133906\n",
      "Batch: 115 / 125 Loss: 2.7418048\n",
      "Batch: 116 / 125 Loss: 2.5108905\n",
      "Batch: 117 / 125 Loss: 2.7513297\n",
      "Batch: 118 / 125 Loss: 2.9199176\n",
      "Batch: 119 / 125 Loss: 2.9487262\n",
      "Batch: 120 / 125 Loss: 3.3103456\n",
      "Batch: 121 / 125 Loss: 2.945041\n",
      "Batch: 122 / 125 Loss: 2.7532566\n",
      "Batch: 123 / 125 Loss: 2.6072059\n",
      "Batch: 124 / 125 Loss: 2.533219\n",
      "Batch: 125 / 125 Loss: 2.5888941\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 21.307805%. Word accuracy: 55.696429%.\n",
      "character error rate : 0.213078047889\n",
      "Character error rate not improved\n",
      "Epoch:============================================================== 18\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 3.1628177\n",
      "Batch: 2 / 125 Loss: 2.5536315\n",
      "Batch: 3 / 125 Loss: 2.4485586\n",
      "Batch: 4 / 125 Loss: 3.2197523\n",
      "Batch: 5 / 125 Loss: 2.711278\n",
      "Batch: 6 / 125 Loss: 2.7166464\n",
      "Batch: 7 / 125 Loss: 2.4225876\n",
      "Batch: 8 / 125 Loss: 3.0135083\n",
      "Batch: 9 / 125 Loss: 2.8019102\n",
      "Batch: 10 / 125 Loss: 2.9836743\n",
      "Batch: 11 / 125 Loss: 2.211641\n",
      "Batch: 12 / 125 Loss: 2.5042858\n",
      "Batch: 13 / 125 Loss: 2.215907\n",
      "Batch: 14 / 125 Loss: 2.412872\n",
      "Batch: 15 / 125 Loss: 2.6792307\n",
      "Batch: 16 / 125 Loss: 2.4978766\n",
      "Batch: 17 / 125 Loss: 3.0675564\n",
      "Batch: 18 / 125 Loss: 2.6451478\n",
      "Batch: 19 / 125 Loss: 2.7943835\n",
      "Batch: 20 / 125 Loss: 2.349661\n",
      "Batch: 21 / 125 Loss: 2.8925698\n",
      "Batch: 22 / 125 Loss: 2.159411\n",
      "Batch: 23 / 125 Loss: 1.9180653\n",
      "Batch: 24 / 125 Loss: 2.6755795\n",
      "Batch: 25 / 125 Loss: 2.6372278\n",
      "Batch: 26 / 125 Loss: 2.448899\n",
      "Batch: 27 / 125 Loss: 2.0292969\n",
      "Batch: 28 / 125 Loss: 2.2282608\n",
      "Batch: 29 / 125 Loss: 2.4383416\n",
      "Batch: 30 / 125 Loss: 2.6223307\n",
      "Batch: 31 / 125 Loss: 2.0871472\n",
      "Batch: 32 / 125 Loss: 1.744776\n",
      "Batch: 33 / 125 Loss: 2.47237\n",
      "Batch: 34 / 125 Loss: 2.2770743\n",
      "Batch: 35 / 125 Loss: 2.895949\n",
      "Batch: 36 / 125 Loss: 2.9992552\n",
      "Batch: 37 / 125 Loss: 2.9367774\n",
      "Batch: 38 / 125 Loss: 2.4718077\n",
      "Batch: 39 / 125 Loss: 2.363205\n",
      "Batch: 40 / 125 Loss: 2.6598508\n",
      "Batch: 41 / 125 Loss: 2.5407836\n",
      "Batch: 42 / 125 Loss: 2.597576\n",
      "Batch: 43 / 125 Loss: 2.386416\n",
      "Batch: 44 / 125 Loss: 2.541069\n",
      "Batch: 45 / 125 Loss: 2.873313\n",
      "Batch: 46 / 125 Loss: 2.5741074\n",
      "Batch: 47 / 125 Loss: 1.8947194\n",
      "Batch: 48 / 125 Loss: 2.2223291\n",
      "Batch: 49 / 125 Loss: 2.8400316\n",
      "Batch: 50 / 125 Loss: 2.8976908\n",
      "Batch: 51 / 125 Loss: 2.3520226\n",
      "Batch: 52 / 125 Loss: 1.7013714\n",
      "Batch: 53 / 125 Loss: 2.675256\n",
      "Batch: 54 / 125 Loss: 2.2978938\n",
      "Batch: 55 / 125 Loss: 2.0875874\n",
      "Batch: 56 / 125 Loss: 1.8828877\n",
      "Batch: 57 / 125 Loss: 2.797458\n",
      "Batch: 58 / 125 Loss: 2.5754168\n",
      "Batch: 59 / 125 Loss: 2.851636\n",
      "Batch: 60 / 125 Loss: 2.567691\n",
      "Batch: 61 / 125 Loss: 3.0522363\n",
      "Batch: 62 / 125 Loss: 2.6610837\n",
      "Batch: 63 / 125 Loss: 2.0602465\n",
      "Batch: 64 / 125 Loss: 2.138871\n",
      "Batch: 65 / 125 Loss: 2.9132419\n",
      "Batch: 66 / 125 Loss: 2.4915504\n",
      "Batch: 67 / 125 Loss: 2.3376832\n",
      "Batch: 68 / 125 Loss: 2.6126237\n",
      "Batch: 69 / 125 Loss: 2.234239\n",
      "Batch: 70 / 125 Loss: 3.0204973\n",
      "Batch: 71 / 125 Loss: 2.5260038\n",
      "Batch: 72 / 125 Loss: 2.7108707\n",
      "Batch: 73 / 125 Loss: 2.6872723\n",
      "Batch: 74 / 125 Loss: 3.182604\n",
      "Batch: 75 / 125 Loss: 2.2785926\n",
      "Batch: 76 / 125 Loss: 2.8561487\n",
      "Batch: 77 / 125 Loss: 2.985867\n",
      "Batch: 78 / 125 Loss: 3.139525\n",
      "Batch: 79 / 125 Loss: 2.4998958\n",
      "Batch: 80 / 125 Loss: 2.7833915\n",
      "Batch: 81 / 125 Loss: 2.5728748\n",
      "Batch: 82 / 125 Loss: 2.255794\n",
      "Batch: 83 / 125 Loss: 3.174655\n",
      "Batch: 84 / 125 Loss: 2.1016924\n",
      "Batch: 85 / 125 Loss: 2.9302487\n",
      "Batch: 86 / 125 Loss: 2.6872797\n",
      "Batch: 87 / 125 Loss: 2.8264549\n",
      "Batch: 88 / 125 Loss: 2.2303386\n",
      "Batch: 89 / 125 Loss: 2.425255\n",
      "Batch: 90 / 125 Loss: 2.973238\n",
      "Batch: 91 / 125 Loss: 2.1063433\n",
      "Batch: 92 / 125 Loss: 2.5729992\n",
      "Batch: 93 / 125 Loss: 2.7682288\n",
      "Batch: 94 / 125 Loss: 2.671775\n",
      "Batch: 95 / 125 Loss: 3.0031037\n",
      "Batch: 96 / 125 Loss: 2.5694437\n",
      "Batch: 97 / 125 Loss: 2.4893076\n",
      "Batch: 98 / 125 Loss: 2.2731013\n",
      "Batch: 99 / 125 Loss: 2.994494\n",
      "Batch: 100 / 125 Loss: 2.2363927\n",
      "Batch: 101 / 125 Loss: 2.014418\n",
      "Batch: 102 / 125 Loss: 2.4427817\n",
      "Batch: 103 / 125 Loss: 2.8409243\n",
      "Batch: 104 / 125 Loss: 2.3636198\n",
      "Batch: 105 / 125 Loss: 2.8592777\n",
      "Batch: 106 / 125 Loss: 2.2446008\n",
      "Batch: 107 / 125 Loss: 2.4346461\n",
      "Batch: 108 / 125 Loss: 2.8921676\n",
      "Batch: 109 / 125 Loss: 2.632383\n",
      "Batch: 110 / 125 Loss: 2.349989\n",
      "Batch: 111 / 125 Loss: 2.7716088\n",
      "Batch: 112 / 125 Loss: 3.2805102\n",
      "Batch: 113 / 125 Loss: 2.3650057\n",
      "Batch: 114 / 125 Loss: 1.5149194\n",
      "Batch: 115 / 125 Loss: 2.4630363\n",
      "Batch: 116 / 125 Loss: 2.608277\n",
      "Batch: 117 / 125 Loss: 2.6772528\n",
      "Batch: 118 / 125 Loss: 2.8479774\n",
      "Batch: 119 / 125 Loss: 2.6445217\n",
      "Batch: 120 / 125 Loss: 2.6959686\n",
      "Batch: 121 / 125 Loss: 3.3295813\n",
      "Batch: 122 / 125 Loss: 2.3849328\n",
      "Batch: 123 / 125 Loss: 2.5707624\n",
      "Batch: 124 / 125 Loss: 2.9729114\n",
      "Batch: 125 / 125 Loss: 2.3907015\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 19.356608%. Word accuracy: 58.964286%.\n",
      "character error rate : 0.193566075672\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 19\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 2.4371493\n",
      "Batch: 2 / 125 Loss: 2.2391844\n",
      "Batch: 3 / 125 Loss: 2.8855202\n",
      "Batch: 4 / 125 Loss: 3.35745\n",
      "Batch: 5 / 125 Loss: 1.8360478\n",
      "Batch: 6 / 125 Loss: 2.4787388\n",
      "Batch: 7 / 125 Loss: 2.95674\n",
      "Batch: 8 / 125 Loss: 2.3898458\n",
      "Batch: 9 / 125 Loss: 2.576706\n",
      "Batch: 10 / 125 Loss: 2.269574\n",
      "Batch: 11 / 125 Loss: 2.5058978\n",
      "Batch: 12 / 125 Loss: 2.8352978\n",
      "Batch: 13 / 125 Loss: 3.3999505\n",
      "Batch: 14 / 125 Loss: 1.813095\n",
      "Batch: 15 / 125 Loss: 2.4767923\n",
      "Batch: 16 / 125 Loss: 2.6072812\n",
      "Batch: 17 / 125 Loss: 1.8205146\n",
      "Batch: 18 / 125 Loss: 2.5564682\n",
      "Batch: 19 / 125 Loss: 3.3661466\n",
      "Batch: 20 / 125 Loss: 2.638732\n",
      "Batch: 21 / 125 Loss: 2.415851\n",
      "Batch: 22 / 125 Loss: 2.0907974\n",
      "Batch: 23 / 125 Loss: 2.443905\n",
      "Batch: 24 / 125 Loss: 3.2037797\n",
      "Batch: 25 / 125 Loss: 2.3184922\n",
      "Batch: 26 / 125 Loss: 2.510745\n",
      "Batch: 27 / 125 Loss: 2.6803873\n",
      "Batch: 28 / 125 Loss: 2.375726\n",
      "Batch: 29 / 125 Loss: 2.482467\n",
      "Batch: 30 / 125 Loss: 2.3273435\n",
      "Batch: 31 / 125 Loss: 2.4430387\n",
      "Batch: 32 / 125 Loss: 2.3602555\n",
      "Batch: 33 / 125 Loss: 2.991879\n",
      "Batch: 34 / 125 Loss: 2.8668256\n",
      "Batch: 35 / 125 Loss: 2.4216437\n",
      "Batch: 36 / 125 Loss: 2.4079494\n",
      "Batch: 37 / 125 Loss: 2.5027397\n",
      "Batch: 38 / 125 Loss: 3.4088125\n",
      "Batch: 39 / 125 Loss: 2.4995768\n",
      "Batch: 40 / 125 Loss: 2.5601084\n",
      "Batch: 41 / 125 Loss: 2.5685487\n",
      "Batch: 42 / 125 Loss: 2.5926723\n",
      "Batch: 43 / 125 Loss: 2.5082912\n",
      "Batch: 44 / 125 Loss: 2.6404574\n",
      "Batch: 45 / 125 Loss: 2.714155\n",
      "Batch: 46 / 125 Loss: 2.313626\n",
      "Batch: 47 / 125 Loss: 2.0238833\n",
      "Batch: 48 / 125 Loss: 2.471552\n",
      "Batch: 49 / 125 Loss: 2.6751509\n",
      "Batch: 50 / 125 Loss: 2.005448\n",
      "Batch: 51 / 125 Loss: 2.5445778\n",
      "Batch: 52 / 125 Loss: 2.8611507\n",
      "Batch: 53 / 125 Loss: 2.4690711\n",
      "Batch: 54 / 125 Loss: 2.6361413\n",
      "Batch: 55 / 125 Loss: 1.7107807\n",
      "Batch: 56 / 125 Loss: 2.3285375\n",
      "Batch: 57 / 125 Loss: 2.4008913\n",
      "Batch: 58 / 125 Loss: 2.5531313\n",
      "Batch: 59 / 125 Loss: 2.3920975\n",
      "Batch: 60 / 125 Loss: 2.4558337\n",
      "Batch: 61 / 125 Loss: 2.3285286\n",
      "Batch: 62 / 125 Loss: 2.5979855\n",
      "Batch: 63 / 125 Loss: 2.5331547\n",
      "Batch: 64 / 125 Loss: 2.1496487\n",
      "Batch: 65 / 125 Loss: 2.4349482\n",
      "Batch: 66 / 125 Loss: 2.9875512\n",
      "Batch: 67 / 125 Loss: 2.6174738\n",
      "Batch: 68 / 125 Loss: 3.086444\n",
      "Batch: 69 / 125 Loss: 2.5990689\n",
      "Batch: 70 / 125 Loss: 1.8826437\n",
      "Batch: 71 / 125 Loss: 3.0773022\n",
      "Batch: 72 / 125 Loss: 2.71628\n",
      "Batch: 73 / 125 Loss: 2.506772\n",
      "Batch: 74 / 125 Loss: 2.331361\n",
      "Batch: 75 / 125 Loss: 2.4970703\n",
      "Batch: 76 / 125 Loss: 2.4824212\n",
      "Batch: 77 / 125 Loss: 2.3305461\n",
      "Batch: 78 / 125 Loss: 2.635036\n",
      "Batch: 79 / 125 Loss: 2.9575982\n",
      "Batch: 80 / 125 Loss: 2.407961\n",
      "Batch: 81 / 125 Loss: 2.8956847\n",
      "Batch: 82 / 125 Loss: 3.0614219\n",
      "Batch: 83 / 125 Loss: 2.3000557\n",
      "Batch: 84 / 125 Loss: 2.8247726\n",
      "Batch: 85 / 125 Loss: 2.3009295\n",
      "Batch: 86 / 125 Loss: 1.8790203\n",
      "Batch: 87 / 125 Loss: 2.1767526\n",
      "Batch: 88 / 125 Loss: 2.175973\n",
      "Batch: 89 / 125 Loss: 2.925209\n",
      "Batch: 90 / 125 Loss: 2.5982828\n",
      "Batch: 91 / 125 Loss: 2.784726\n",
      "Batch: 92 / 125 Loss: 2.7201667\n",
      "Batch: 93 / 125 Loss: 2.3555963\n",
      "Batch: 94 / 125 Loss: 2.3553383\n",
      "Batch: 95 / 125 Loss: 2.3376074\n",
      "Batch: 96 / 125 Loss: 2.1890075\n",
      "Batch: 97 / 125 Loss: 2.3393855\n",
      "Batch: 98 / 125 Loss: 2.245313\n",
      "Batch: 99 / 125 Loss: 2.2616029\n",
      "Batch: 100 / 125 Loss: 3.1614454\n",
      "Batch: 101 / 125 Loss: 2.0014162\n",
      "Batch: 102 / 125 Loss: 2.0957336\n",
      "Batch: 103 / 125 Loss: 2.5738153\n",
      "Batch: 104 / 125 Loss: 2.3633606\n",
      "Batch: 105 / 125 Loss: 2.3621218\n",
      "Batch: 106 / 125 Loss: 2.5542536\n",
      "Batch: 107 / 125 Loss: 1.9028461\n",
      "Batch: 108 / 125 Loss: 2.0283206\n",
      "Batch: 109 / 125 Loss: 2.6447842\n",
      "Batch: 110 / 125 Loss: 2.1830618\n",
      "Batch: 111 / 125 Loss: 3.0142634\n",
      "Batch: 112 / 125 Loss: 2.672141\n",
      "Batch: 113 / 125 Loss: 2.4625583\n",
      "Batch: 114 / 125 Loss: 2.3784516\n",
      "Batch: 115 / 125 Loss: 2.0350575\n",
      "Batch: 116 / 125 Loss: 1.8656518\n",
      "Batch: 117 / 125 Loss: 2.2956343\n",
      "Batch: 118 / 125 Loss: 2.6082363\n",
      "Batch: 119 / 125 Loss: 2.7242575\n",
      "Batch: 120 / 125 Loss: 2.2663674\n",
      "Batch: 121 / 125 Loss: 2.6146429\n",
      "Batch: 122 / 125 Loss: 2.8445148\n",
      "Batch: 123 / 125 Loss: 2.75615\n",
      "Batch: 124 / 125 Loss: 2.4237804\n",
      "Batch: 125 / 125 Loss: 2.093377\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 18.451837%. Word accuracy: 59.517857%.\n",
      "character error rate : 0.184518369585\n",
      "Character error rate improved, save model\n",
      "Epoch:============================================================== 20\n",
      "Train NN\n",
      "Batch: 1 / 125 Loss: 1.6614364\n",
      "Batch: 2 / 125 Loss: 2.2219148\n",
      "Batch: 3 / 125 Loss: 2.2422602\n",
      "Batch: 4 / 125 Loss: 2.6179645\n",
      "Batch: 5 / 125 Loss: 2.4801655\n",
      "Batch: 6 / 125 Loss: 2.4321394\n",
      "Batch: 7 / 125 Loss: 2.3794246\n",
      "Batch: 8 / 125 Loss: 2.1212\n",
      "Batch: 9 / 125 Loss: 2.0153642\n",
      "Batch: 10 / 125 Loss: 2.7104914\n",
      "Batch: 11 / 125 Loss: 2.1103776\n",
      "Batch: 12 / 125 Loss: 2.1260798\n",
      "Batch: 13 / 125 Loss: 2.3471618\n",
      "Batch: 14 / 125 Loss: 2.4513178\n",
      "Batch: 15 / 125 Loss: 2.405609\n",
      "Batch: 16 / 125 Loss: 2.4464192\n",
      "Batch: 17 / 125 Loss: 2.7589211\n",
      "Batch: 18 / 125 Loss: 2.698922\n",
      "Batch: 19 / 125 Loss: 3.4940188\n",
      "Batch: 20 / 125 Loss: 2.3778188\n",
      "Batch: 21 / 125 Loss: 2.3641367\n",
      "Batch: 22 / 125 Loss: 2.1270607\n",
      "Batch: 23 / 125 Loss: 2.8549075\n",
      "Batch: 24 / 125 Loss: 2.8366241\n",
      "Batch: 25 / 125 Loss: 2.7364883\n",
      "Batch: 26 / 125 Loss: 2.5778594\n",
      "Batch: 27 / 125 Loss: 1.9793425\n",
      "Batch: 28 / 125 Loss: 2.2643058\n",
      "Batch: 29 / 125 Loss: 2.7719734\n",
      "Batch: 30 / 125 Loss: 2.2514029\n",
      "Batch: 31 / 125 Loss: 2.930379\n",
      "Batch: 32 / 125 Loss: 2.548622\n",
      "Batch: 33 / 125 Loss: 3.2088764\n",
      "Batch: 34 / 125 Loss: 2.3733735\n",
      "Batch: 35 / 125 Loss: 2.4126687\n",
      "Batch: 36 / 125 Loss: 2.9019828\n",
      "Batch: 37 / 125 Loss: 2.4619186\n",
      "Batch: 38 / 125 Loss: 2.9452055\n",
      "Batch: 39 / 125 Loss: 2.594339\n",
      "Batch: 40 / 125 Loss: 2.4359815\n",
      "Batch: 41 / 125 Loss: 2.5371888\n",
      "Batch: 42 / 125 Loss: 2.1747282\n",
      "Batch: 43 / 125 Loss: 2.218451\n",
      "Batch: 44 / 125 Loss: 2.6218357\n",
      "Batch: 45 / 125 Loss: 2.360257\n",
      "Batch: 46 / 125 Loss: 2.4074442\n",
      "Batch: 47 / 125 Loss: 2.0365717\n",
      "Batch: 48 / 125 Loss: 2.4564528\n",
      "Batch: 49 / 125 Loss: 2.0522392\n",
      "Batch: 50 / 125 Loss: 3.0534623\n",
      "Batch: 51 / 125 Loss: 2.9358006\n",
      "Batch: 52 / 125 Loss: 2.2430782\n",
      "Batch: 53 / 125 Loss: 2.5541933\n",
      "Batch: 54 / 125 Loss: 2.816233\n",
      "Batch: 55 / 125 Loss: 1.9829216\n",
      "Batch: 56 / 125 Loss: 2.517725\n",
      "Batch: 57 / 125 Loss: 2.4997065\n",
      "Batch: 58 / 125 Loss: 2.0865254\n",
      "Batch: 59 / 125 Loss: 2.2497501\n",
      "Batch: 60 / 125 Loss: 1.9663478\n",
      "Batch: 61 / 125 Loss: 2.0265334\n",
      "Batch: 62 / 125 Loss: 1.7913344\n",
      "Batch: 63 / 125 Loss: 2.659883\n",
      "Batch: 64 / 125 Loss: 2.3577082\n",
      "Batch: 65 / 125 Loss: 2.2167525\n",
      "Batch: 66 / 125 Loss: 2.2071548\n",
      "Batch: 67 / 125 Loss: 3.0104547\n",
      "Batch: 68 / 125 Loss: 2.546914\n",
      "Batch: 69 / 125 Loss: 2.300335\n",
      "Batch: 70 / 125 Loss: 2.655345\n",
      "Batch: 71 / 125 Loss: 2.17194\n",
      "Batch: 72 / 125 Loss: 1.9925047\n",
      "Batch: 73 / 125 Loss: 2.3911598\n",
      "Batch: 74 / 125 Loss: 2.5152705\n",
      "Batch: 75 / 125 Loss: 2.5584657\n",
      "Batch: 76 / 125 Loss: 2.7563698\n",
      "Batch: 77 / 125 Loss: 2.2277603\n",
      "Batch: 78 / 125 Loss: 2.0899575\n",
      "Batch: 79 / 125 Loss: 2.5016286\n",
      "Batch: 80 / 125 Loss: 2.5580938\n",
      "Batch: 81 / 125 Loss: 2.3695426\n",
      "Batch: 82 / 125 Loss: 2.4074578\n",
      "Batch: 83 / 125 Loss: 2.6388712\n",
      "Batch: 84 / 125 Loss: 2.384081\n",
      "Batch: 85 / 125 Loss: 2.4151006\n",
      "Batch: 86 / 125 Loss: 2.5404122\n",
      "Batch: 87 / 125 Loss: 3.2239718\n",
      "Batch: 88 / 125 Loss: 2.2951238\n",
      "Batch: 89 / 125 Loss: 2.383592\n",
      "Batch: 90 / 125 Loss: 2.5736752\n",
      "Batch: 91 / 125 Loss: 2.1369927\n",
      "Batch: 92 / 125 Loss: 2.245813\n",
      "Batch: 93 / 125 Loss: 2.5151713\n",
      "Batch: 94 / 125 Loss: 2.1435444\n",
      "Batch: 95 / 125 Loss: 2.4658093\n",
      "Batch: 96 / 125 Loss: 2.304977\n",
      "Batch: 97 / 125 Loss: 2.2961411\n",
      "Batch: 98 / 125 Loss: 2.0279374\n",
      "Batch: 99 / 125 Loss: 2.0470495\n",
      "Batch: 100 / 125 Loss: 2.5894701\n",
      "Batch: 101 / 125 Loss: 2.7941036\n",
      "Batch: 102 / 125 Loss: 2.5412753\n",
      "Batch: 103 / 125 Loss: 2.6381466\n",
      "Batch: 104 / 125 Loss: 2.6717293\n",
      "Batch: 105 / 125 Loss: 2.9938204\n",
      "Batch: 106 / 125 Loss: 1.9861653\n",
      "Batch: 107 / 125 Loss: 2.3807385\n",
      "Batch: 108 / 125 Loss: 2.1690147\n",
      "Batch: 109 / 125 Loss: 2.7830508\n",
      "Batch: 110 / 125 Loss: 2.1051512\n",
      "Batch: 111 / 125 Loss: 2.2736914\n",
      "Batch: 112 / 125 Loss: 2.2728853\n",
      "Batch: 113 / 125 Loss: 2.492215\n",
      "Batch: 114 / 125 Loss: 2.2551389\n",
      "Batch: 115 / 125 Loss: 2.157924\n",
      "Batch: 116 / 125 Loss: 2.4656825\n",
      "Batch: 117 / 125 Loss: 2.4333363\n",
      "Batch: 118 / 125 Loss: 2.953234\n",
      "Batch: 119 / 125 Loss: 2.9779656\n",
      "Batch: 120 / 125 Loss: 2.35229\n",
      "Batch: 121 / 125 Loss: 2.5904834\n",
      "Batch: 122 / 125 Loss: 2.3989596\n",
      "Batch: 123 / 125 Loss: 2.0887563\n",
      "Batch: 124 / 125 Loss: 2.454049\n",
      "Batch: 125 / 125 Loss: 2.569089\n",
      "validation starts =============\n",
      "Validate NN\n",
      "Character error rate: 18.506672%. Word accuracy: 59.767857%.\n",
      "character error rate : 0.185066715409\n",
      "Character error rate not improved\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-43fc0b9cf510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-43fc0b9cf510>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m#validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoderType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmustRestore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                         \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e240f5212233>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, charList, decoderType, mustRestore)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# setup CNN, RNN and CTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetupCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetupRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetupCTC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e240f5212233>\u001b[0m in \u001b[0;36msetupRNN\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# bidirectional RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;31m# BxTxF -> BxTx2H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional_dynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_fw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_bw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnnIn3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnnIn3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    441\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3554\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3556\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3557\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 3087\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   3088\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3089\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   3021\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3523\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3524\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    845\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    846\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;31m# Keras cells always wrap state as list, even if it's a single tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_rnn_cell\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# In graph mode, failure to build the layer's graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# implies a user-side bug. We don't catch exceptions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1513\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1514\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 371\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[0;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/tf_utils.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         partitioner=maybe_partitioner)\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1230\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;34m\"\"\"Alias for `add_weight`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             getter=vs.get_variable)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/checkpointable/base.pyc\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    605\u001b[0m     new_variable = getter(\n\u001b[1;32m    606\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    528\u001b[0m           function_utils.has_kwargs(custom_getter)):\n\u001b[1;32m    529\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m_rnn_get_variable\u001b[0;34m(self, getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[0;32m--> 848\u001b[0;31m             traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-23-e240f5212233>\", line 87, in setupRNN\n    ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d, dtype=rnnIn3d.dtype)\n  File \"<ipython-input-23-e240f5212233>\", line 38, in __init__\n    self.setupRNN()\n  File \"<ipython-input-29-43fc0b9cf510>\", line 32, in main\n    model = Model(loader.charList, decoderType)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\t\"main function\"\n",
    "\t# optional command line args\n",
    "# \tparser = argparse.ArgumentParser()\n",
    "# \tparser.add_argument(\"--train\", help=\"train the NN\", action=\"store_true\")\n",
    "# \tparser.add_argument(\"--validate\", help=\"validate the NN\", action=\"store_true\")\n",
    "# \tparser.add_argument(\"--beamsearch\", help=\"use beam search instead of best path decoding\", action=\"store_true\")\n",
    "# \tparser.add_argument(\"--wordbeamsearch\", help=\"use word beam search instead of best path decoding\", action=\"store_true\")\n",
    "# \targs = parser.parse_args()\n",
    "\ttf.reset_default_graph()\n",
    "\n",
    "\tdecoderType = DecoderType.BestPath\n",
    "# \tif args.beamsearch:\n",
    "# \t\tdecoderType = DecoderType.BeamSearch\n",
    "# \telif args.wordbeamsearch:\n",
    "# \t\tdecoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "\t# train or validate on IAM dataset\t\n",
    "\tif  True:\n",
    "\t\t# load training data, create TF model\n",
    "\t\tloader = DataLoader(FilePaths.fnTrain, Model.batchSize, Model.imgSize, Model.maxTextLen)\n",
    "\n",
    "\t\t# save characters of model for inference mode\n",
    "\t\topen(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\n",
    "\t\t\n",
    "\t\t# save words contained in dataset into file\n",
    "\t\topen(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "\t\t# execute training or validation\n",
    "\t\tif True:\n",
    "\t\t\tmodel = Model(loader.charList, decoderType)\n",
    "\t\t\ttrain(model, loader)\n",
    "      #validate\n",
    "\t\t\tmodel = Model(loader.charList, decoderType, mustRestore=True)\n",
    "\t\t\tvalidate(model, loader)\n",
    "\n",
    "\t# infer text on test image\n",
    "\telse:\n",
    "\t\tprint(open(FilePaths.fnAccuracy).read())\n",
    "\t\tmodel = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True)\n",
    "\t\tinfer(model, FilePaths.fnInfer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akkzmITfJgCC"
   },
   "outputs": [],
   "source": [
    "\n",
    "files.download('default.txt') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjGjyrxRrp2l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hwnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
